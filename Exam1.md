در پاسخ به پرسش شما در مورد **سوال ۱ منبع exam1** و با تشریح کامل مراحل حل ریاضی، به جزئیات هر بخش می‌پردازیم. جدول داده مورد نظر از منبع [۲] به شرح زیر است:

| A (x_1) | B (x_2) | Label (y) |
| :-----: | :-----: | :-------: |
|    1    |    0    |     0     |
|    0    |    1    |     0     |
|   -1    |    0    |     1     |
|    0    |   -1    |     1     |

در این جدول، `A` و `B` ویژگی‌های ورودی (`x_1`, `x_2`) و `Label` متغیر هدف (`y`) است.

---

### الف) طبقه‌بندی رگرسیون لجستیک (Logistic Regression Classifier) برای جدول داده

رگرسیون لجستیک یک مدل طبقه‌بندی است که احتمال تعلق یک نمونه به یک کلاس خاص را تخمین می‌زند [۹۰, ۸۴]. مراحل ریاضی آن به شرح زیر است:

1.  **محاسبه ورودی خالص (Net Input)**
    ورودی خالص `z` برای هر نمونه `i` به صورت یک ترکیب خطی از ویژگی‌های ورودی (`x_j`)، وزن‌های مدل (`w_j`) و یک جمله بایاس (`b`) محاسبه می‌شود [۴۱]:

    `z^(i) = w_1 * x_1^(i) + w_2 * x_2^(i) + b` [۴۱]

    برای داده‌های جدول فوق:
    *   نمونه ۱ (A=1, B=0, Label=0): `z^(1) = w_1 * (1) + w_2 * (0) + b = w_1 + b`
    *   نمونه ۲ (A=0, B=1, Label=0): `z^(2) = w_1 * (0) + w_2 * (1) + b = w_2 + b`
    *   نمونه ۳ (A=-1, B=0, Label=1): `z^(3) = w_1 * (-1) + w_2 * (0) + b = -w_1 + b`
    *   نمونه ۴ (A=0, B=-1, Label=1): `z^(4) = w_1 * (0) + w_2 * (-1) + b = -w_2 + b`

2.  **تابع سیگموئید (Logistic Sigmoid Function)**
    مقدار `z` به تابع سیگموئید (یا لجستیک) وارد می‌شود. این تابع `z` را به یک مقدار احتمال `σ(z)` بین ۰ و ۱ نگاشت می‌کند [۸۸]:

    `σ(z) = 1 / (1 + e^(-z))` [۸۸]

    خروجی `σ(z)` به عنوان احتمال تعلق نمونه به **کلاس ۱** (`p(y=1|x)`) تفسیر می‌شود [۹۰]. برای مثال، اگر `σ(z) = 0.8` باشد، ۸۰ درصد احتمال تعلق به کلاس ۱ وجود دارد [۹۰].

3.  **تصمیم‌گیری طبقه‌بندی (Classification Decision)**
    برای تبدیل احتمال به برچسب نهایی کلاس (۰ یا ۱)، یک آستانه تصمیم‌گیری (معمولاً ۰.۵) اعمال می‌شود [۹۱]:
    *   اگر `σ(z) ≥ 0.5` باشد، مدل پیش‌بینی می‌کند که نمونه متعلق به **کلاس ۱** است [۹۱].
    *   در غیر این صورت (اگر `σ(z) < 0.5`)، مدل پیش‌بینی می‌کند که نمونه متعلق به **کلاس ۰** است [۹۱].
    این شرط همچنین معادل با این است که اگر `z ≥ 0` باشد، `y_pred = 1` و در غیر این صورت `y_pred = 0` [۹۱].

4.  **تابع زیان (Loss Function) - کراس‌انتروپی لجستیک**
    برای آموزش مدل و یافتن بهترین وزن‌ها و بایاس، از تابع زیان کراس‌انتروپی (که به آن **تابع زیان لجستیک** نیز می‌گویند) استفاده می‌شود [۹۲, ۹۴]. هدف حداقل کردن این تابع زیان است:

    `L(w, b) = Σ[−y^(i) log(σ(z^(i))) − (1 − y^(i)) log(1 − σ(z^(i)))]` [۹۴]

    در این فرمول:
    *   `y^(i)`: برچسب واقعی کلاس برای نمونه `i` (۰ یا ۱) [۹۴].
    *   `σ(z^(i))`: احتمال پیش‌بینی شده برای تعلق نمونه `i` به کلاس ۱ [۹۴].
    این تابع به گونه‌ای طراحی شده است که **پیش‌بینی‌های نادرست را با زیان بیشتری جریمه می‌کند** [۹۹].

---

### ب) منظم‌سازی (Regularization)

منظم‌سازی یک روش حیاتی برای مقابله با پدیده **بیش‌برازش (overfitting)** است، که در آن مدل به جای یادگیری الگوهای کلی، جزئیات و نویز داده‌های آموزشی را حفظ می‌کند و در داده‌های جدید عملکرد ضعیفی دارد [۷۴, ۱۲۲]. منظم‌سازی با افزودن یک **جمله جریمه (penalty term)** به تابع زیان اصلی عمل می‌کند [۷۴].

**انواع متداول منظم‌سازی**:

1.  **منظم‌سازی L1 (LASSO - Least Absolute Shrinkage and Selection Operator)**
    این روش یک جمله جریمه به تابع زیان اضافه می‌کند که متناسب با **مجموع قدر مطلق وزن‌ها** است [۱۳۶]:

    `Penalty_L1 = λ * Σ|w_j|`

    تابع زیان نهایی با منظم‌سازی L1:
    `L_total = L(w, b) + λ * Σ|w_j|`

    ویژگی بارز L1 این است که می‌تواند برخی از وزن‌ها را دقیقاً به صفر برساند و در نتیجه به **انتخاب ویژگی (feature selection)** منجر شود و مدل‌های تنک‌تر (sparse models) ایجاد کند [۱۳۶].

2.  **منظم‌سازی L2 (Ridge Regression)**
    این روش یک جمله جریمه به تابع زیان اضافه می‌کند که متناسب با **مجموع مربع وزن‌ها** است [۱۳۵, ۱۳۶]:

    `Penalty_L2 = λ * Σ(w_j)^2`

    تابع زیان نهایی با منظم‌سازی L2:
    `L_total = L(w, b) + λ * Σ(w_j)^2`

    منظم‌سازی L2 وزن‌ها را به سمت صفر کوچک می‌کند، اما معمولاً آن‌ها را کاملاً صفر نمی‌کند [۱۳۶].

**پارامتر منظم‌سازی (`λ` یا `C`)**:
قدرت منظم‌سازی توسط یک هایپرپارامتر (مانند `λ` در Ridge/Lasso یا `C` در Scikit-learn که معکوس قدرت منظم‌سازی است) کنترل می‌شود [۷۴, ۱۰۵, ۲۶۶]. **افزایش قدرت منظم‌سازی (افزایش `λ` یا کاهش `C`) بایاس مدل را افزایش و واریانس (بیش‌برازش) را کاهش می‌دهد** [۱۱۰]. انتخاب مناسب این پارامتر برای تعادل بین بایاس و واریانس مدل ضروری است.

---

### ج) محاسبه ضرایب با استفاده از کاهنده گرادیان (Gradient Descent)

کاهنده گرادیان (Gradient Descent) یک الگوریتم بهینه‌سازی تکراری است که برای **یافتن پارامترهای مدل (وزن‌ها `w` و بایاس `b`) که تابع زیان را به حداقل می‌رسانند**، استفاده می‌شود [۶۳, ۳۰۴].

**مراحل کلی الگوریتم کاهنده گرادیان**:

1.  **مقداردهی اولیه پارامترها**:
    وزن‌های `w` و بایاس `b` مدل با مقادیر اولیه (معمولاً صفر یا اعداد تصادفی کوچک) مقداردهی می‌شوند [۴۳, ۵۲].

2.  **تکرار (Epochs)**:
    الگوریتم برای تعداد مشخصی از تکرارها ("epochs") روی مجموعه داده آموزشی اجرا می‌شود. در هر epoch، مدل کل داده‌های آموزشی را پردازش می‌کند [۴۳].

3.  **بروزرسانی پارامترها (Parameter Update)**:
    در هر گام، پارامترها در **جهت مخالف گرادیان تابع زیان (`L_total`)** (جهت تندترین کاهش تابع) بروزرسانی می‌شوند [۶۵, ۳۰۴]. اندازه گامی که در این جهت برداشته می‌شود توسط **نرخ یادگیری (learning rate - `η`)** کنترل می‌شود [۴۴]:

    **قوانین بروزرسانی عمومی**:
    `w_j := w_j - η * ∂L_total/∂w_j` [۲, ۳۰۴]
    `b := b - η * ∂L_total/∂b` [۲, ۳۰۴]

    *   **محاسبه گرادیان (مشتقات جزئی)**:

        *   **برای تابع زیان لجستیک (بدون منظم‌سازی)**:
            مشتق جزئی تابع زیان لجستیک `L` نسبت به وزن `w_j` به صورت زیر است [۱۰۳]:
            `∂L/∂w_j = Σ[(σ(z^(i)) - y^(i)) * x_j^(i)]`

            مشتق جزئی تابع زیان لجستیک `L` نسبت به بایاس `b` به صورت زیر است [۱۰۳]:
            `∂L/∂b = Σ[(σ(z^(i)) - y^(i))]`

            (این مشتقات جزئی برای رگرسیون لجستیک مشابه مشتقات آدالاین هستند، با این تفاوت که خروجی فعال‌سازی `a^(i)` با `σ(z^(i))` جایگزین می‌شود [۱۰۳].)

        *   **برای منظم‌سازی L2 (Ridge)**:
            اگر `Penalty_L2 = λ * Σ(w_j)^2` باشد، مشتق جزئی آن نسبت به `w_j` برابر است با:
            `∂Penalty_L2/∂w_j = 2 * λ * w_j`
            بایاس (`b`) معمولاً منظم‌سازی نمی‌شود، بنابراین مشتق جمله جریمه نسبت به آن صفر است.

        *   **برای منظم‌سازی L1 (Lasso)**:
            اگر `Penalty_L1 = λ * Σ|w_j|` باشد، مشتق جزئی آن نسبت به `w_j` برابر است با:
            `∂Penalty_L1/∂w_j = λ * sign(w_j)`
            (توجه داشته باشید که تابع `sign(w_j)` در `w_j = 0` تعریف‌نشده است و در عمل با روش‌های خاصی مدیریت می‌شود).

    *   **فرمول‌های بروزرسانی با منظم‌سازی L2 (مثال کامل)**:
        با ترکیب تابع زیان لجستیک و جمله جریمه L2، تابع زیان کلی `L_total = L(w, b) + λ * Σ(w_j)^2` می‌شود.
        بنابراین، قوانین بروزرسانی پارامترها عبارتند از:
        `w_j := w_j - η * ( Σ[(σ(z^(i)) - y^(i)) * x_j^(i)] + 2 * λ * w_j )`
        `b := b - η * Σ[(σ(z^(i)) - y^(i))]`

**نرخ یادگیری (`η`)**:
انتخاب مناسب نرخ یادگیری حیاتی است. اگر `η` خیلی بزرگ باشد، الگوریتم ممکن است از حداقل سراسری "پرش" کند و به همگرایی نرسد یا واگرا شود. اگر `η` خیلی کوچک باشد، همگرایی بسیار کند خواهد بود و آموزش زمان زیادی می‌برد [۷۰].

**انواع کاهنده گرادیان**:
*   **گرادیان دیسنت دسته‌ای کامل (Full Batch Gradient Descent)**: گرادیان بر اساس کل مجموعه داده آموزشی محاسبه می‌شود [۶۵].
*   **گرادیان دیسنت تصادفی (Stochastic Gradient Descent - SGD)**: گرادیان تنها بر اساس یک نمونه آموزشی واحد در هر مرحله محاسبه می‌شود [۳۱۸].
*   **گرادیان دیسنت مینی‌بچ (Mini-batch Gradient Descent)**: گرادیان بر اساس یک زیرمجموعه کوچک (mini-batch) از داده‌های آموزشی محاسبه می‌شود [۳۱۸]. این روش تعادلی بین سرعت SGD و دقت Batch Gradient Descent برقرار می‌کند و معمولاً در عمل ترجیح داده می‌شود [۳۱۸].

با دنبال کردن این مراحل، می‌توان پارامترهای بهینه برای مدل رگرسیون لجستیک را پیدا کرد و آن را برای طبقه‌بندی داده‌ها آموزش داد.
