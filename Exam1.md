این متن به صورت Markdown برای GitHub آماده شده است.

---

# رگرسیون لجستیک و کاهنده گرادیان: حل گام به گام

در پاسخ به پرسش شما در مورد **سوال ۱ منبع exam1** و با تشریح کامل مراحل حل ریاضی به همراه محاسبات گام به گام و ارائه جواب نهایی، از داده‌های ارائه شده در **منبع [۲]** استفاده می‌کنیم:

| A ($x_1$) | B ($x_2$) | Label (y) |
| :-------: | :-------: | :-------: |
| 1 | 0 | 0 |
| 0 | 1 | 0 |
| -1 | 0 | 1 |
| 0 | -1 | 1 |

هدف این است که با استفاده از **طبقه‌بندی رگرسیون لجستیک (Logistic Regression)**، ضرایب ($w$ و $b$) را از طریق **کاهنده گرادیان (Gradient Descent)** محاسبه کنیم [۱].

---

## الف) طبقه‌بندی رگرسیون لجستیک

رگرسیون لجستیک احتمال تعلق یک نمونه به کلاس مثبت (۱) را تخمین می‌زند [۹۰, ۸۴]. مراحل آن:

1.  **ورودی خالص (Net Input) - z:**
    $z = w_1 * x_1 + w_2 * x_2 + b$ [۴۱]
2.  **تابع سیگموئید (Sigmoid Function) - $\sigma(z)$:**
    $\sigma(z) = \frac{1}{(1 + e^{-z})}$ [۸۸]

    این تابع خروجی $z$ را به مقادیر احتمالی بین ۰ و ۱ تبدیل می‌کند [۸۸]. $\sigma(z)$ به عنوان $p(y=1|x)$ (احتمال تعلق به کلاس ۱) تفسیر می‌شود [۹۰].
3.  **تصمیم‌گیری طبقه‌بندی:**
    اگر $\sigma(z) \geq 0.5$ باشد، مدل پیش‌بینی می‌کند که نمونه متعلق به کلاس ۱ است [۹۱]. در غیر این صورت، مدل پیش‌بینی می‌کند که نمونه متعلق به کلاس ۰ است [۹۱]. این معادل است با: اگر $z \geq 0$، $y_{pred} = 1$ و در غیر این صورت $y_{pred} = 0$ [۹۱].
4.  **تابع زیان (Loss Function) - کراس‌انتروپی لجستیک:**
    برای آموزش مدل، تابع زیان کراس‌انتروپی را به حداقل می‌رسانیم [۹۲, ۹۴]:
    $L(w, b) = \Sigma[−y^{(i)} \log(\sigma(z^{(i)})) − (1 − y^{(i)}) \log(1 − \sigma(z^{(i)}))]$ [۹۴]
    این تابع پیش‌بینی‌های نادرست را با زیان بیشتری جریمه می‌کند [۹۹].

---

## ب) منظم‌سازی (Regularization)

**منظم‌سازی** روشی برای مقابله با **بیش‌برازش (overfitting)** است که مدل را قادر می‌سازد تا به داده‌های جدید نیز خوب تعمیم یابد [۷۴, ۱۲۲]. این کار با افزودن یک جمله جریمه (penalty term) به تابع زیان اصلی انجام می‌شود [۷۴].

**انواع متداول منظم‌سازی:**
* **منظم‌سازی L1 (LASSO):** جمله جریمه متناسب با مجموع قدر مطلق وزن‌ها است ($\lambda * \Sigma|w_j|$). این روش می‌تواند برخی از وزن‌ها را به صفر برساند و به انتخاب ویژگی کمک می‌کند [۱۳۶].
* **منظم‌سازی L2 (Ridge Regression):** جمله جریمه متناسب با مجموع مربع وزن‌ها است ($\lambda * \Sigma(w_j)^2$). این روش وزن‌ها را به سمت صفر کوچک می‌کند اما معمولاً آن‌ها را کاملاً صفر نمی‌کند [۱۳۵, ۱۳۶].

**پارامتر منظم‌سازی ($\lambda$ یا C):**
قدرت منظم‌سازی توسط یک **هایپرپارامتر** کنترل می‌شود [۷۴, ۱۰۵]. افزایش قدرت منظم‌سازی (افزایش $\lambda$ یا کاهش C) **بایاس** مدل را افزایش و **واریانس (بیش‌برازش)** را کاهش می‌دهد [۱۱۰].

---

## ج) محاسبه ضرایب با استفاده از کاهنده گرادیان (Gradient Descent)

کاهنده گرادیان یک **الگوریتم بهینه‌سازی تکراری** است که برای یافتن پارامترهای مدل (وزن‌ها $w$ و بایاس $b$) که تابع زیان را به حداقل می‌رسانند، استفاده می‌شود [۶۳, ۳۰۴].

**مراحل کلی الگوریتم:**
1.  **مقداردهی اولیه پارامترها:** وزن‌ها و بایاس با مقادیر اولیه (معمولاً صفر یا اعداد تصادفی کوچک) مقداردهی می‌شوند [۴۳, ۵۲].
2.  **تکرار (Epochs):** الگوریتم برای تعداد مشخصی از تکرارها اجرا می‌شود [۴۳].
3.  **بروزرسانی پارامترها:** پارامترها در جهت مخالف گرادیان تابع زیان (جهت تندترین کاهش تابع) بروزرسانی می‌شوند [۶۵, ۳۰۴]. **نرخ یادگیری ($\eta$)** اندازه این گام را کنترل می‌کند [۴۴].

**فرمول‌های بروزرسانی (بدون منظم‌سازی برای سادگی محاسبات دستی) [۲, ۱۰۳]:**
$\frac{\partial L}{\partial w_j} = \Sigma[(\sigma(z^{(i)}) - y^{(i)}) * x_j^{(i)}]$
$\frac{\partial L}{\partial b} = \Sigma[(\sigma(z^{(i)}) - y^{(i)})]$

$w_j := w_j - \eta * \frac{\partial L}{\partial w_j}$
$b := b - \eta * \frac{\partial L}{\partial b}$

---

## محاسبات گام به گام و جواب نهایی

برای انجام محاسبات، فرضیات زیر را در نظر می‌گیریم:
* **مقادیر اولیه پارامترها:** $w_1 = 0$، $w_2 = 0$، $b = 0$ [۴۳, ۵۲]
* **نرخ یادگیری (Learning Rate):** $\eta = 0.1$ (یک مقدار رایج در مثال‌ها [۴۹, ۶۸, ۷۵])
* **منظم‌سازی:** در این محاسبات گام به گام، برای سادگی و وضوح، جمله جریمه منظم‌سازی را در نظر نمی‌گیریم. (اگر منظم‌سازی L2 اعمال می‌شد، جملات $2 * \lambda * w_j$ به مشتقات $w_j$ اضافه می‌شدند [۱۰۵].)

**داده‌ها:**
| Sample (i) | $x_1$ | $x_2$ | y |
| :---------: | :-: | :-: | :-: |
| 1 | 1 | 0 | 0 |
| 2 | 0 | 1 | 0 |
| 3 | -1 | 0 | 1 |
| 4 | 0 | -1 | 1 |

### **Epoch 1:**

**گام ۱: محاسبه $z^{(i)}$ و $\sigma(z^{(i)})$ و $(\sigma(z^{(i)}) - y^{(i)})$ برای هر نمونه**
* **نمونه ۱ ($x_1 = 1, x_2 = 0, y = 0$):**
    $z^{(1)} = (0 * 1) + (0 * 0) + 0 = 0$
    $\sigma(z^{(1)}) = \frac{1}{(1 + e^0)} = \frac{1}{(1 + 1)} = 0.5$
    $Error\_term^{(1)} = \sigma(z^{(1)}) - y^{(1)} = 0.5 - 0 = 0.5$
* **نمونه ۲ ($x_1 = 0, x_2 = 1, y = 0$):**
    $z^{(2)} = (0 * 0) + (0 * 1) + 0 = 0$
    $\sigma(z^{(2)}) = \frac{1}{(1 + e^0)} = 0.5$
    $Error\_term^{(2)} = \sigma(z^{(2)}) - y^{(2)} = 0.5 - 0 = 0.5$
* **نمونه ۳ ($x_1 = -1, x_2 = 0, y = 1$):**
    $z^{(3)} = (0 * -1) + (0 * 0) + 0 = 0$
    $\sigma(z^{(3)}) = \frac{1}{(1 + e^0)} = 0.5$
    $Error\_term^{(3)} = \sigma(z^{(3)}) - y^{(3)} = 0.5 - 1 = -0.5$
* **نمونه ۴ ($x_1 = 0, x_2 = -1, y = 1$):**
    $z^{(4)} = (0 * 0) + (0 * -1) + 0 = 0$
    $\sigma(z^{(4)}) = \frac{1}{(1 + e^0)} = 0.5$
    $Error\_term^{(4)} = \sigma(z^{(4)}) - y^{(4)} = 0.5 - 1 = -0.5$

**گام ۲: محاسبه گرادیان‌ها**
$\frac{\partial L}{\partial w_1} = (0.5 * 1) + (0.5 * 0) + (-0.5 * -1) + (-0.5 * 0) = 0.5 + 0 + 0.5 + 0 = 1.0$
$\frac{\partial L}{\partial w_2} = (0.5 * 0) + (0.5 * 1) + (-0.5 * 0) + (-0.5 * -1) = 0 + 0.5 + 0 + 0.5 = 1.0$
$\frac{\partial L}{\partial b} = 0.5 + 0.5 + (-0.5) + (-0.5) = 0$

**گام ۳: بروزرسانی پارامترها**
$w_1 := w_1 - \eta * \frac{\partial L}{\partial w_1} = 0 - (0.1 * 1.0) = -0.1$
$w_2 := w_2 - \eta * \frac{\partial L}{\partial w_2} = 0 - (0.1 * 1.0) = -0.1$
$b := b - \eta * \frac{\partial L}{\partial b} = 0 - (0.1 * 0) = 0$

**پارامترهای پس از Epoch 1:**
* $w_1 = -0.1$
* $w_2 = -0.1$
* $b = 0$

### **Epoch 2:**

**گام ۱: محاسبه $z^{(i)}$ و $\sigma(z^{(i)})$ و $(\sigma(z^{(i)}) - y^{(i)})$ برای هر نمونه (با استفاده از $w_1 = -0.1, w_2 = -0.1, b = 0$)**
* **نمونه ۱ ($x_1 = 1, x_2 = 0, y = 0$):**
    $z^{(1)} = (-0.1 * 1) + (-0.1 * 0) + 0 = -0.1$
    $\sigma(z^{(1)}) = \frac{1}{(1 + e^{-(-0.1)})} = \frac{1}{(1 + e^{0.1})} \approx \frac{1}{(1 + 1.105)} \approx 0.475$
    $Error\_term^{(1)} = 0.475 - 0 = 0.475$
* **نمونه ۲ ($x_1 = 0, x_2 = 1, y = 0$):**
    $z^{(2)} = (-0.1 * 0) + (-0.1 * 1) + 0 = -0.1$
    $\sigma(z^{(2)}) \approx 0.475$
    $Error\_term^{(2)} = 0.475 - 0 = 0.475$
* **نمونه ۳ ($x_1 = -1, x_2 = 0, y = 1$):**
    $z^{(3)} = (-0.1 * -1) + (-0.1 * 0) + 0 = 0.1$
    $\sigma(z^{(3)}) = \frac{1}{(1 + e^{-0.1})} \approx \frac{1}{(1 + 0.905)} \approx 0.525$
    $Error\_term^{(3)} = 0.525 - 1 = -0.475$
* **نمونه ۴ ($x_1 = 0, x_2 = -1, y = 1$):**
    $z^{(4)} = (-0.1 * 0) + (-0.1 * -1) + 0 = 0.1$
    $\sigma(z^{(4)}) \approx 0.525$
    $Error\_term^{(4)} = 0.525 - 1 = -0.475$

**گام ۲: محاسبه گرادیان‌ها**
$\frac{\partial L}{\partial w_1} = (0.475 * 1) + (0.475 * 0) + (-0.475 * -1) + (-0.475 * 0) = 0.475 + 0 + 0.475 + 0 = 0.95$
$\frac{\partial L}{\partial w_2} = (0.475 * 0) + (0.475 * 1) + (-0.475 * 0) + (-0.475 * -1) = 0 + 0.475 + 0 + 0.475 = 0.95$
$\frac{\partial L}{\partial b} = 0.475 + 0.475 + (-0.475) + (-0.475) = 0$

**گام ۳: بروزرسانی پارامترها**
$w_1 := w_1 - \eta * \frac{\partial L}{\partial w_1} = -0.1 - (0.1 * 0.95) = -0.1 - 0.095 = -0.195$
$w_2 := w_2 - \eta * \frac{\partial L}{\partial w_2} = -0.1 - (0.1 * 0.95) = -0.1 - 0.095 = -0.195$
$b := b - \eta * \frac{\partial L}{\partial b} = 0 - (0.1 * 0) = 0$

**پارامترهای پس از Epoch 2:**
* $w_1 = -0.195$
* $w_2 = -0.195$
* $b = 0$

---

## جواب نهایی و تحلیل

با توجه به ماهیت تکراری الگوریتم کاهنده گرادیان، "جواب نهایی" به معنای مقادیری از پارامترها است که پس از تعداد کافی تکرار (epochs) به یک مقدار حداقل همگرا شده‌اند [۴۹, ۶۳]. در اینجا، پس از دو دور تکرار، مقادیر ضرایب به شرح زیر است:

* **ضریب $w_1$ (برای ویژگی A) = -0.195**
* **ضریب $w_2$ (برای ویژگی B) = -0.195**
* **بایاس $b$ = 0**

همانطور که مشاهده می‌شود، وزن‌ها $w_1$ و $w_2$ در حال کاهش (منفی‌تر شدن) هستند، در حالی که بایاس $b$ همچنان صفر باقی مانده است. این نشان‌دهنده همگرایی تدریجی پارامترها در جهت کاهش تابع زیان است [۶۹]. صفر ماندن بایاس در این مثال خاص به دلیل تقارن داده‌ها حول محور y=0 و مقادیر اولیه صفر برای وزن‌ها و بایاس است، که منجر به مشتق صفر بایاس در هر گام می‌شود. در یک اجرای کامل، این فرآیند برای تعداد مشخصی از epochs (مثلاً ۵۰ یا ۱۰۰۰ دور) ادامه می‌یابد تا پارامترها به مقادیر بهینه همگرا شوند [۴۹, ۱۰۱].

برای افزایش دقت و جلوگیری از بیش‌برازش، می‌توان از تکنیک‌های منظم‌سازی (مانند L1 یا L2) استفاده کرد. در صورت استفاده از منظم‌سازی L2، قوانین بروزرسانی وزن‌ها ($w_j$) شامل یک جمله اضافی $2 * \lambda * w_j$ در گرادیان خواهد بود، که وزن‌ها را به سمت صفر کوچک می‌کند [۱۰۵, ۱۳۶]. انتخاب مناسب نرخ یادگیری نیز برای همگرایی بهینه بسیار مهم است؛ نرخ یادگیری بیش از حد بزرگ می‌تواند باعث پرش از حداقل شود، در حالی که نرخ یادگیری بسیار کوچک باعث کندی همگرایی می‌شود [۶۹, ۷۰].





---

**سوال ۲ –**  
داده‌های جدول زیر را در نظر بگیرید. با استفاده از روش **ماشین بردار پشتیبان (SVM)** برای طبقه‌بندی داده‌ها، به سؤالات زیر پاسخ دهید.

### جدول داده‌ها:

| \( i \)       | \( X_1 \) | \( X_2 \) | \( Y \) | \( \alpha_i \) |
|---------------|----------|----------|--------|----------------|
| \( x_1 \)     | 8.0      | 5.8      | 1      | 0.414          |
| \( x_2 \)     | 8.0      | 8.0      | 1      | 0.000          |
| \( x_3 \)     | 2.0      | 5.0      | -1     | 0.000          |
| \( x_4 \)     | 5.0      | 2.0      | -1     | 0.018          |
| \( x_5 \)     | 9.8      | 9.0      | 1      | 0.000          |
| \( x_6 \)     | 3.8      | 3.8      | -1     | 0.000          |
| \( x_7 \)     | 7.0      | 8.0      | 1      | 0.018          |
| \( x_8 \)     | 1.0      | 3.0      | -1     | 0.000          |
| \( x_9 \)     | 4.0      | 4.2      | -1     | 0.414          |
| \( x_{10} \)  | 9.0      | 5.0      | 1      | 0.000          |

---

### فرضیات:
- مدل SVM با **تابع تصمیم خطی** \( h(\mathbf{x}) \) آموزش داده شده است.
- هدف، محاسبه **مارژین (Margin)** و بررسی وضعیت نقاط نسبت به مرز تصمیم است.
- همچنین، مقدار \( h(\mathbf{x}) \) برای نقطهٔ تست \( \mathbf{x} = (3, 3)^T \) باید محاسبه شود.

---

### سؤالات:

**الف)** تابع تصمیم \( h(\mathbf{x}) \) را برای این مدل SVM به دست آورید.  
نحوه محاسبه آن را با استفاده از فرمول زیر و داده‌های جدول (به‌ویژه نقاط با \( \alpha_i > 0 \)) توضیح دهید:

\[
h(\mathbf{x}) = \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i^T \mathbf{x} + b
\]

در اینجا، فقط نقاطی که \( \alpha_i > 0 \) هستند (یعنی **بردارهای پشتیبان**) در محاسبه تأثیر دارند.

---

**ب)** فاصله نقطه \( x_6 = (3.8, 3.8)^T \) نسبت به مرز تصمیم (یعنی مقدار **مارژین نسبی** \( \delta \)) را محاسبه کنید.  
آیا این نقطه یک **بردار پشتیبان (Support Vector)** است؟ چرا؟

راهنمایی: از فرمول زیر استفاده کنید:

\[
\delta = \frac{y_i \cdot h(\mathbf{x}_i)}{\|\mathbf{w}\|}
\]

---

**پ)** مقدار تابع تصمیم \( h(\mathbf{x}) \) را برای نقطهٔ تست \( \mathbf{x} = (3, 3)^T \) محاسبه کنید.  
سپس، برچسب پیش‌بینی‌شدهٔ آن (مثبت یا منفی) را مشخص کنید.

---

✅ **نکته:**  
- بردار وزن \( \mathbf{w} \) از رابطه \( \mathbf{w} = \sum \alpha_i y_i \mathbf{x}_i \) به دست می‌آید.  
- مقدار \( b \) (بایاس) را می‌توان با استفاده از یکی از بردارهای پشتیبان (مثلاً \( x_1 \) یا \( x_9 \)) و رابطه \( h(\mathbf{x}_i) = y_i \) محاسبه کرد.

---
```markdown
### الف) تابع تصمیم \( h(\mathbf{x}) \)

تابع تصمیم برای یک SVM خطی به شکل  
\[
h(\mathbf{x}) = \mathbf{w}^\mathsf{T}\mathbf{x} + b
\]  
تعریف می‌شود. بردار وزن \( \mathbf{w} \) و بایاس \( b \) با استفاده از بردارهای پشتیبان و ضرایب \( \alpha_i \) محاسبه می‌شوند.

#### شناسایی بردارهای پشتیبان
نمونه‌هایی که \( \alpha_i > 0 \) هستند:

| نقطه | مختصات | برچسب \( y_i \) | \( \alpha_i \) |
|------|---------|-----------------|----------------|
| \( \mathbf{x}_1 \) | \( (8, 5.8) \) | \( +1 \) | 0.414 |
| \( \mathbf{x}_4 \) | \( (5, 2) \)   | \( -1 \) | 0.018 |
| \( \mathbf{x}_7 \) | \( (7, 8) \)   | \( +1 \) | 0.018 |
| \( \mathbf{x}_9 \) | \( (4, 4.2) \) | \( -1 \) | 0.414 |

#### محاسبه بردار وزن \( \mathbf{w} \)
\[
\mathbf{w} = \sum_{i \in \text{SV}} \alpha_i y_i \mathbf{x}_i
\]

\[
\begin{aligned}
\mathbf{w} &= 0.414(+1)\begin{pmatrix}8 \\ 5.8\end{pmatrix}
            + 0.018(-1)\begin{pmatrix}5 \\ 2\end{pmatrix}
            + 0.018(+1)\begin{pmatrix}7 \\ 8\end{pmatrix}
            + 0.414(-1)\begin{pmatrix}4 \\ 4.2\end{pmatrix} \\[4pt]
&= \begin{pmatrix}3.312 \\ 2.3952\end{pmatrix}
 + \begin{pmatrix}-0.090 \\ -0.036\end{pmatrix}
 + \begin{pmatrix}0.126 \\ 0.144\end{pmatrix}
 + \begin{pmatrix}-1.656 \\ -1.7388\end{pmatrix} \\[4pt]
&= \begin{pmatrix}1.692 \\ 0.7644\end{pmatrix}
\end{aligned}
\]

#### محاسبه بایاس \( b \)
با استفاده از \( \mathbf{x}_4 \) (که \( \alpha \) کوچک‌تری دارد):

\[
-1 \cdot \bigl(1.692 \cdot 5 + 0.7644 \cdot 2 + b\bigr) = 1
\Longrightarrow b = -10.9888
\]

بنابراین تابع تصمیم:

\[
\boxed{h(\mathbf{x}) = 1.692\,x_1 + 0.7644\,x_2 - 10.9888}
\]

---

### ب) فاصله (مارژین) نقطه \( \mathbf{x}_6 \)

- **وضعیت \( \mathbf{x}_6 \):**  
  \( \alpha_6 = 0 \Rightarrow \) **بردار پشتیبان نیست**.

- **محاسبه \( h(\mathbf{x}_6) \):**  
  \[
  h\!\bigl((3.8, 3.8)\bigr) = 1.692 \cdot 3.8 + 0.7644 \cdot 3.8 - 10.9888 \approx -1.6545
  \]

- **نرم بردار وزن:**  
  \[
  \|\mathbf{w}\| = \sqrt{1.692^2 + 0.7644^2} \approx 1.8566
  \]

- **مارژین:**  
  \[
  \delta_6 = \frac{(-1)\cdot(-1.6545)}{1.8566} \approx 0.8911
  \]

\[
\boxed{\delta_6 \approx 0.891 \quad (\text{SV نیست})}
\]

---

### پ) طبقه‌بندی نقطه \( (3,3)^\mathsf{T} \)

\[
h\!\bigl((3,3)\bigr) = 1.692 \cdot 3 + 0.7644 \cdot 3 - 10.9888 \approx -3.6196 < 0
\]

\[
\boxed{(3,3) \text{ در کلاس } -1 \text{ قرار می‌گیرد}}
\]
```

بله، در ادامه متن شما **کاملاً بدون تغییر** و تنها با **فرمت‌بندی صحیح عبارات ریاضی با استفاده از LaTeX** (مانند `$...$` برای فرمول‌های درون خط و `$$...$$` یا `\( ... \)` و `\[ ... \]` برای فرمول‌های جدا) ارائه می‌شود. تنها تغییر، جایگزینی علامت‌گذاری ریاضی برای نمایش بهتر فرمول‌ها است:

---

بله، برای حل مجدد مسئله تحلیل تفکیک خطی (LDA) بدون مرحله استانداردسازی، مراحل را مستقیماً روی داده‌های اصلی انجام می‌دهیم. استانداردسازی، که در پاسخ قبلی اشاره شد [۱۰۶]، ویژگی‌ها را به میانگین صفر و واریانس یک مقیاس می‌کند، اما LDA ذاتاً با داده‌های غیر استاندارد نیز کار می‌کند.

**مراحل LDA بدون استانداردسازی:**

1.  **محاسبه بردارهای میانگین برای هر کلاس:** میانگین ویژگی‌های $X_1$ و $X_2$ را برای هر کلاس $Y=0$ و $Y=1$ محاسبه می‌کنیم.
2.  **محاسبه میانگین کلی (Overall Mean):** میانگین هر ویژگی را برای کل مجموعه داده محاسبه می‌کنیم.
3.  **ساخت ماتریس پراکندگی درون کلاسی $S_W$:** این ماتریس مجموع پراکندگی نمونه‌ها را حول میانگین کلاس‌های خودشان اندازه‌گیری می‌کند [۱۴۵].
4.  **ساخت ماتریس پراکندگی بین کلاسی $S_B$:** این ماتریس پراکندگی میانگین کلاس‌ها را حول میانگین کلی داده‌ها اندازه‌گیری می‌کند [۱۴۹].
5.  **محاسبه مقادیر ویژه (Eigenvalues) و بردارهای ویژه (Eigenvectors) ماتریس $S_W^{-1} S_B$:** این مرحله به ما کمک می‌کند تا جهت‌هایی را پیدا کنیم که جدایی‌پذیری کلاس‌ها را به حداکثر می‌رسانند [۱۵۰].
6.  **انتخاب بردار ویژه اصلی:** از آنجایی که دو کلاس $Y=0$ و $Y=1$ داریم، تنها یک بردار ویژه اصلی $(c-1)$ انتخاب می‌شود که بیشترین اطلاعات تفکیک‌کننده را دارد [۱۵۱، ۱۴۲]. این بردار ویژه، ماتریس تبدیل $W$ خواهد بود.
7.  **تصویرسازی (Projection) نمونه‌ها بر روی فضای ویژگی جدید:** داده‌های اصلی را با استفاده از ماتریس تبدیل $W$ بر روی یک فضای تک‌بعدی جدید تصویر می‌کنیم $X_{\text{LDA}} = XW$ [۱۵۵].

---

**محاسبات با استفاده از داده‌های جدول (بدون استانداردسازی):**

**جدول داده‌ها:**

| X1   | X2   | Y  |
|------|------|----|
| 2.1  | 0.8  | 1  |
| 1.5  | 0.3  | 0  |
| 0.7  | 1.2  | 1  |
| 0.9  | 0.5  | 0  |
| 1.8  | 1.0  | 1  |
| 0.3  | 0.6  | 0  |

**۱. محاسبه بردارهای میانگین برای هر کلاس و میانگین کلی:**

*   **داده‌های کلاس $Y=0$:** $\left[1.5, 0.3\right], \left[0.9, 0.5\right], \left[0.3, 0.6\right]$
*   **میانگین کلاس $Y=0$ ($\mu_0$):** $\left[\frac{1.5+0.9+0.3}{3}, \frac{0.3+0.5+0.6}{3}\right] = \left[0.9, 0.4667\right]$
*   **داده‌های کلاس $Y=1$:** $\left[2.1, 0.8\right], \left[0.7, 1.2\right], \left[1.8, 1.0\right]$
*   **میانگین کلاس $Y=1$ ($\mu_1$):** $\left[\frac{2.1+0.7+1.8}{3}, \frac{0.8+1.2+1.0}{3}\right] = \left[1.5333, 1.0\right]$
*   **میانگین کلی ($\mu_{\text{overall}}$):** $\left[\frac{0.9 \times 3 + 1.5333 \times 3}{6}, \frac{0.4667 \times 3 + 1.0 \times 3}{6}\right] = \left[1.2167, 0.7333\right]$

**۲. ساخت ماتریس پراکندگی درون کلاسی $S_W$:**

*   **برای کلاس $Y=0$ ($S_{W_0}$):**
    \[
    S_{W_0} \approx \begin{pmatrix} 0.7200 & -0.1800 \\ -0.1800 & 0.0467 \end{pmatrix}
    \]
*   **برای کلاس $Y=1$ ($S_{W_1}$):**
    \[
    S_{W_1} \approx \begin{pmatrix} 1.0867 & -0.2800 \\ -0.2800 & 0.0800 \end{pmatrix}
    \]
*   **ماتریس پراکندگی درون کلاسی کل ($S_W$):**
    \[
    S_W = S_{W_0} + S_{W_1} \approx \begin{pmatrix} 1.8067 & -0.4600 \\ -0.4600 & 0.1267 \end{pmatrix}
    \]

**۳. ساخت ماتریس پراکندگی بین کلاسی $S_B$:**

*   $\mu_0 - \mu_{\text{overall}} = [0.9 - 1.2167, 0.4667 - 0.7333] = [-0.3167, -0.2666]$
*   $\mu_1 - \mu_{\text{overall}} = [1.5333 - 1.2167, 1.0 - 0.7333] = [0.3166, 0.2667]$
*   $S_B = 3 (\mu_0 - \mu_{\text{overall}})(\mu_0 - \mu_{\text{overall}})^T + 3 (\mu_1 - \mu_{\text{overall}})(\mu_1 - \mu_{\text{overall}})^T$
*   \[
    S_B \approx \begin{pmatrix} 0.6015 & 0.5064 \\ 0.5064 & 0.4266 \end{pmatrix}
    \]

**۴. محاسبه مقادیر ویژه و بردارهای ویژه ماتریس $S_W^{-1} S_B$:**

*   ابتدا معکوس $S_W$ را محاسبه می‌کنیم:
    \[
    S_W^{-1} \approx \begin{pmatrix} 7.3237 & 26.5900 \\ 26.5900 & 104.4277 \end{pmatrix}
    \]
*   سپس ماتریس $S_W^{-1} S_B$ را به دست می‌آوریم:
    \[
    M = S_W^{-1} S_B \approx \begin{pmatrix} 17.865 & 15.058 \\ 68.885 & 58.010 \end{pmatrix}
    \]
*   **مقادیر ویژه (Eigenvalues):**
    یکی از مقادیر ویژه تقریباً $75.87$ و دیگری تقریباً $0$ است. این نشان‌دهنده وجود یک جهت تفکیک‌کننده معنی‌دار است [۱۵۱].
*   **بردار ویژه (Eigenvector) اصلی:**
    بردار ویژه مربوط به بزرگترین مقدار ویژه ($\approx 75.87$) تقریباً برابر است با:
    \[
    W = \begin{pmatrix} 0.207 \\ 0.978 \end{pmatrix}
    \]

**۵. تصویرسازی نمونه‌ها بر روی فضای ویژگی جدید:**
داده‌های اصلی را با ضرب در ماتریس $W$ بر روی یک متغیر جدید (تک‌بعدی) تصویر می‌کنیم: $X_{\text{LDA}} = X \cdot W$.

*   **مقادیر متغیر جدید $X_{\text{LDA}}$ برای هر نقطه:**

| X1   | X2   | Y  | $X_{\text{LDA}}$ (متغیر جدید) |
|------|------|----|-------------------------------|
| 2.1  | 0.8  | 1  | $2.1 \times 0.207 + 0.8 \times 0.978 \approx 1.2171$ |
| 1.5  | 0.3  | 0  | $1.5 \times 0.207 + 0.3 \times 0.978 \approx 0.6039$ |
| 0.7  | 1.2  | 1  | $0.7 \times 0.207 + 1.2 \times 0.978 \approx 1.3185$ |
| 0.9  | 0.5  | 0  | $0.9 \times 0.207 + 0.5 \times 0.978 \approx 0.6753$ |
| 1.8  | 1.0  | 1  | $1.8 \times 0.207 + 1.0 \times 0.978 \approx 1.3506$ |
| 0.3  | 0.6  | 0  | $0.3 \times 0.207 + 0.6 \times 0.978 \approx 0.6489$ |

**متغیر جدید ایجاد شده $X_{\text{LDA}}$:**

| نقطه | Y  | $X_{\text{LDA}}$ (متغیر جدید) |
|-----|----|-------------------------------|
| ۱   | ۱  | $1.2171$                      |
| ۲   | ۰  | $0.6039$                      |
| ۳   | ۱  | $1.3185$                      |
| ۴   | ۰  | $0.6753$                      |
| ۵   | ۱  | $1.3506$                      |
| ۶   | ۰  | $0.6489$                      |

**نتیجه‌گیری:**
همانطور که مشاهده می‌شود، مقادیر متغیر جدید $X_{\text{LDA}}$ برای کلاس $Y=1$ (تقریباً $1.22$، $1.32$، $1.35$) به طور کلی بالاتر از مقادیر برای کلاس $Y=0$ (تقریباً $0.60$، $0.68$، $0.65$) هستند. این نتایج نشان می‌دهند که حتی بدون استانداردسازی، LDA یک متغیر جدید ایجاد کرده است که می‌تواند این دو کلاس را به طور خطی از هم جدا کند [۱۳۸، ۱۶۱]. مرز تصمیم‌گیری می‌تواند در نقطه‌ای بین این دو گروه از مقادیر، مثلاً در حدود $X_{\text{LDA}} = 1.0$، قرار گیرد.

--- 





---

با سؤال ۴ 

**نکته مهم در مورد تفسیر جدول:**  
ساختار جدول ارائه شده برای مقادیر $TP$, $FN$, $FP$, $TN$ در نگاه اول کمی نامتعارف است. با این حال، با در نظر گرفتن نحوه استفاده از این مقادیر در فرمول‌های محاسبه دقت (Accuracy)، فراخوانی (Recall) و دقت (Precision)، ما فرض می‌کنیم که چهار عدد ارائه شده برای هر سطح پیچیدگی (در ستون‌های P و N) به ترتیب $TP$ (True Positive)، $FN$ (False Negative)، $FP$ (False Positive) و $TN$ (True Negative) هستند. بر این اساس، مجموع کل نمونه‌های مثبت واقعی (P Actual) از $TP + FN$ و مجموع کل نمونه‌های منفی واقعی (N Actual) از $FP + TN$ به دست می‌آید. لازم به ذکر است که این تفسیر ممکن است با مقادیر "Sum" در انتهای جدول در تضاد باشد (مثلاً برای داده‌های آموزشی، $TP + FN$ برابر با 1200 است در حالی که Sum P برابر با 900 ذکر شده است). برای انجام محاسبات درخواستی، ما مقادیر $TP, FN, FP, TN$ که مستقیماً از خانه‌های جدول خوانده می‌شوند را معیار قرار داده و $P_{\text{Actual}}$ و $N_{\text{Actual}}$ را از جمع آنها استخراج می‌کنیم.

---

### **الف) ترسیم منحنی درستی (Accuracy) به میزان پیچیدگی برای داده‌های بادگیری و ارزیابی:**

ابتدا مقادیر $TP$, $FN$, $FP$, $TN$ را برای هر سطح پیچیدگی و برای هر دو مجموعه داده (آموزش و اعتبارسنجی) استخراج می‌کنیم و سپس دقت (Accuracy) را با استفاده از فرمول  
\[
\text{ACC} = \frac{TP + TN}{TP + FN + FP + TN}
\]  
محاسبه می‌کنیم.

**داده‌های آموزشی (Training Data):**

| پیچیدگی | TP  | FN  | FP  | TN  | $P_{\text{Actual}} = TP+FN$ | $N_{\text{Actual}} = FP+TN$ | کل نمونه‌ها | دقت (Accuracy) |
|:--------|:----|:----|:----|:----|:-----------------------------|:-----------------------------|:------------|:---------------|
| 1       | 300 | 900 | 225 | 75  | 1200                         | 300                          | 1500        | 0.25           |
| 2       | 600 | 600 | 150 | 150 | 1200                         | 300                          | 1500        | 0.50           |
| 3       | 900 | 300 | 75  | 225 | 1200                         | 300                          | 1500        | 0.75           |
| 4       | 900 | 300 | 75  | 225 | 1200                         | 300                          | 1500        | 0.75           |

**داده‌های اعتبارسنجی (Validation Data):**

| پیچیدگی | TP  | FN  | FP  | TN  | $P_{\text{Actual}} = TP+FN$ | $N_{\text{Actual}} = FP+TN$ | کل نمونه‌ها | دقت (Accuracy) |
|:--------|:----|:----|:----|:----|:-----------------------------|:-----------------------------|:------------|:---------------|
| 1       | 20  | 100 | 25  | 5   | 120                          | 30                           | 150         | 0.1667         |
| 2       | 55  | 65  | 16  | 14  | 120                          | 30                           | 150         | 0.46           |
| 3       | 85  | 35  | 7   | 23  | 120                          | 30                           | 150         | 0.72           |
| 4       | 85  | 35  | 7   | 23  | 120                          | 30                           | 150         | 0.72           |

**توصیف منحنی درستی:**

- **محور افقی (X-axis):** میزان پیچیدگی مدل (Complexity) از 1 تا 4.  
- **محور عمودی (Y-axis):** دقت (Accuracy) از 0.0 تا 1.0.  
- **خط اول (درستی آموزش):** این خط مقادیر دقت را برای داده‌های آموزشی (0.25، 0.50، 0.75، 0.75) نمایش می‌دهد. مشاهده می‌شود که با افزایش پیچیدگی از 1 به 3، دقت آموزش به طور قابل توجهی افزایش می‌یابد و سپس در پیچیدگی 4 ثابت می‌ماند.  
- **خط دوم (درستی اعتبارسنجی):** این خط مقادیر دقت را برای داده‌های اعتبارسنجی (0.1667، 0.46، 0.72، 0.72) نمایش می‌دهد. مشابه دقت آموزش، دقت اعتبارسنجی نیز با افزایش پیچیدگی تا سطح 3 رشد می‌کند و سپس ثابت می‌ماند.  

به طور کلی، هر دو منحنی نشان می‌دهند که با افزایش پیچیدگی مدل، عملکرد مدل در ابتدا بهبود می‌یابد و سپس به ثبات می‌رسد.

---

### **ب) شناسایی کمترین پیش‌برازش و محاسبه Recall و Precision در آن پیچیدگی:**

**شناسایی کمترین پیش‌برازش (Underfitting):**

پیش‌برازش (Underfiting) زمانی رخ می‌دهد که مدل به اندازه کافی پیچیده نباشد تا الگوهای موجود در داده‌های آموزشی را به خوبی یاد بگیرد و در نتیجه عملکرد پایینی هم در داده‌های آموزشی و هم در داده‌های جدید (اعتبارسنجی) دارد [۶۹، ۱۳۵].  
برای تشخیص کمترین پیش‌برازش، به **بالاترین دقت اعتبارسنجی** با **حداقل فاصله نسبت به دقت آموزشی** نگاه می‌کنیم که نشان‌دهنده یک تعادل خوب بین یادگیری از داده‌های آموزشی و تعمیم‌پذیری به داده‌های جدید است [۱۳۵].

با بررسی مقادیر دقت محاسبه شده:

- **پیچیدگی 1:** دقت آموزش 0.25 و دقت اعتبارسنجی 0.1667. هر دو بسیار پایین هستند که نشان‌دهنده **پیش‌برازش شدید** است.  
- **پیچیدگی 2:** دقت آموزش 0.50 و دقت اعتبارسنجی 0.46. هنوز هم پیش‌برازش وجود دارد، اما کمتر از پیچیدگی 1 است.  
- **پیچیدگی 3:** دقت آموزش 0.75 و دقت اعتبارسنجی 0.72. در این سطح، عملکرد به طور قابل توجهی بهبود یافته است.  
- **پیچیدگی 4:** دقت آموزش 0.75 و دقت اعتبارسنجی 0.72. عملکرد دقیقاً مشابه پیچیدگی 3 است.  

با توجه به اینکه دقت اعتبارسنجی در پیچیدگی 3 (و 4) به حداکثر خود می‌رسد و فاصله بین دقت آموزش و اعتبارسنجی (0.03) نیز نسبتاً کم است، می‌توان گفت که **کمترین پیش‌برازش در پیچیدگی 3 (یا 4) اتفاق می‌افتد**. در این نقطه، مدل به خوبی الگوها را یاد گرفته و به خوبی تعمیم می‌یابد. ما **پیچیدگی 3** را به عنوان نقطه‌ی بهینه انتخاب می‌کنیم.

**محاسبه Recall و Precision برای پیچیدگی 3 (بر روی داده‌های اعتبارسنجی):**

برای پیچیدگی 3 در داده‌های اعتبارسنجی، مقادیر ماتریس درهم‌ریختگی به شرح زیر است:  
- $TP = 85$  
- $FN = 35$  
- $FP = 7$  
- $TN = 23$  

اکنون Recall و Precision را با استفاده از فرمول‌های مربوطه [۱۵۸] محاسبه می‌کنیم:

1.  **Recall (فراخوانی):**  
    \[
    \text{REC} = \frac{TP}{TP + FN}
    \]  
    \[
    \text{REC} = \frac{85}{85 + 35} = \frac{85}{120} = 0.7083
    \]

2.  **Precision (دقت):**  
    \[
    \text{PRE} = \frac{TP}{TP + FP}
    \]  
    \[
    \text{PRE} = \frac{85}{85 + 7} = \frac{85}{92} = 0.9239
    \]

**نتیجه:** در سطح پیچیدگی 3 که مدل کمترین پیش‌برازش را نشان می‌دهد، **Recall حدود $0.7083$** و **Precision حدود $0.9239$** است. این مقادیر نشان می‌دهند که مدل توانایی خوبی در شناسایی نمونه‌های مثبت واقعی (با Recall $0.7083$) و همچنین دقت بالایی در بین نمونه‌هایی که به عنوان مثبت پیش‌بینی کرده است (با Precision $0.9239$) دارد.

---

