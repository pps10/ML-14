در پاسخ به پرسش شما در مورد سوال 1 منبع exam1، به تشریح مراحل حل با استفاده از اطلاعات موجود در منابع می‌پردازیم:

**الف) طبقه‌بندی رگرسیون لجستیک (Logistic Regression Classifier) برای جدول داده**

رگرسیون لجستیک، برخلاف نامش، یک مدل طبقه‌بندی قدرتمند است که عمدتاً برای حل مسائل طبقه‌بندی دودویی (binary classification) به کار می‌رود. هدف اصلی این مدل، **تخمین احتمال تعلق یک نمونه به یک کلاس خاص** است.

برای جدول داده‌ای که در منبع ارائه شده است:
| A | B | Label |
|---|---|-------|
| 1 | 0 | 0     |
| 0 | 1 | 0     |
| -1| 0 | 1     |
| 0 | -1| 1     |

*   **ویژگی‌های ورودی (X):** ستون‌های `A` و `B` به عنوان ویژگی‌های ورودی مدل عمل می‌کنند.
*   **متغیر هدف (Y):** ستون `Label` متغیر هدف است که دارای دو مقدار گسسته 0 و 1 می‌باشد. این ساختار یک مسئله طبقه‌بندی دودویی را نشان می‌دهد.

**مراحل کار رگرسیون لجستیک:**

1.  **محاسبه ورودی خالص (Net Input):** در ابتدا، ورودی خالص `z` برای هر نمونه محاسبه می‌شود. این `z` یک ترکیب خطی از ویژگی‌های ورودی `x` و وزن‌های `w` (ضرایب) به همراه یک جمله بایاس `b` است:
    `z = w_1x_1 + w_2x_2 + ... + w_mx_m + b = w^Tx + b`.
    در اینجا `x_1` و `x_2` همان ویژگی‌های `A` و `B` هستند.

2.  **تابع سیگموئید (Logistic Sigmoid Function):** مقدار `z` سپس به تابع سیگموئید (که به آن تابع لجستیک نیز می‌گویند) وارد می‌شود. این تابع `z` را به یک مقدار احتمال `σ(z)` بین 0 و 1 نگاشت می‌کند:
    `σ(z) = 1 / (1 + e^(-z))`.
    خروجی `σ(z)` به عنوان احتمال تعلق نمونه به **کلاس 1** تفسیر می‌شود، یعنی `p(y=1|x)`.

3.  **تصمیم‌گیری طبقه‌بندی (Classification Decision):** برای تبدیل این احتمال به یک برچسب کلاس نهایی (0 یا 1)، یک آستانه تصمیم‌گیری (معمولاً 0.5) اعمال می‌شود:
    *   اگر `σ(z) ≥ 0.5` باشد، مدل پیش‌بینی می‌کند که نمونه متعلق به **کلاس 1** است.
    *   در غیر این صورت (اگر `σ(z) < 0.5`)، مدل پیش‌بینی می‌کند که نمونه متعلق به **کلاس 0** است.

4.  **تابع زیان (Loss Function):** برای آموزش مدل و یافتن بهترین مقادیر برای وزن‌ها `w` و بایاس `b`، از تابع زیان لجستیک (معروف به تابع کراس‌انتروپی) استفاده می‌شود. هدف این است که تابع زیان را به حداقل برسانیم. این تابع زیان به گونه‌ای طراحی شده است که **پیش‌بینی‌های نادرست را با زیان (loss) بیشتری جریمه می‌کند**.

**ب) منظم‌سازی (Regularization)**

منظم‌سازی یک روش حیاتی در یادگیری ماشین برای مقابله با پدیده **بیش‌برازش (overfitting)** است. بیش‌برازش زمانی رخ می‌دهد که مدل به جای یادگیری الگوهای کلی در داده‌ها، نویز و جزئیات خاص داده‌های آموزشی را حفظ می‌کند و در نتیجه در پیش‌بینی داده‌های جدید و دیده نشده عملکرد ضعیفی از خود نشان می‌دهد.

**نحوه کار منظم‌سازی:**

منظم‌سازی با افزودن یک **جمله جریمه (penalty term)** به تابع زیان اصلی مدل عمل می‌کند. این جمله جریمه، مقادیر پارامترهای مدل (وزن‌ها) را کوچک می‌کند و بدین ترتیب **پیچیدگی مدل را کاهش می‌دهد**.

**انواع متداول منظم‌سازی:**

*   **منظم‌سازی L1 (LASSO - Least Absolute Shrinkage and Selection Operator):** این روش یک جمله جریمه به تابع زیان اضافه می‌کند که متناسب با **مجموع قدر مطلق وزن‌ها** است. ویژگی بارز L1 این است که می‌تواند برخی از وزن‌ها را دقیقاً به صفر برساند و در نتیجه به **انتخاب ویژگی (feature selection)** منجر شود و مدل‌های تنک‌تر (sparse models) ایجاد کند.

*   **منظم‌سازی L2 (Ridge Regression):** این روش یک جمله جریمه به تابع زیان اضافه می‌کند که متناسب با **مجموع مربع وزن‌ها** است. منظم‌سازی L2 وزن‌ها را به سمت صفر کوچک می‌کند، اما معمولاً آن‌ها را کاملاً صفر نمی‌کند.

**اهمیت پارامتر منظم‌سازی:**
قدرت منظم‌سازی توسط یک هایپرپارامتر (مانند `C` در Scikit-learn که معکوس قدرت منظم‌سازی است، یا `α` در Ridge/Lasso) کنترل می‌شود. **افزایش قدرت منظم‌سازی (کاهش `C` یا افزایش `α`) بایاس مدل را افزایش و واریانس (بیش‌برازش) را کاهش می‌دهد**. انتخاب مناسب این پارامتر برای تعادل بین بایاس و واریانس مدل و دستیابی به بهترین عملکرد بر روی داده‌های جدید ضروری است.

**ج) محاسبه ضرایب با استفاده از کاهنده گرادیان (Gradient Descent)**

کاهنده گرادیان (Gradient Descent) یک الگوریتم بهینه‌سازی تکراری است که به طور گسترده برای **یافتن پارامترهای مدل (وزن‌ها و بایاس) که تابع زیان (loss function) را به حداقل می‌رسانند**، استفاده می‌شود.

**مراحل کلی الگوریتم کاهنده گرادیان:**

1.  **مقداردهی اولیه پارامترها:** وزن‌های `w` و بایاس `b` مدل با مقادیر اولیه (معمولاً صفر یا اعداد تصادفی کوچک) مقداردهی می‌شوند.

2.  **تکرار (Epochs):** الگوریتم برای تعداد مشخصی از تکرارها (که به آن‌ها "epochs" گفته می‌شود) روی مجموعه داده آموزشی اجرا می‌شود. در هر epoch، مدل کل داده‌های آموزشی را پردازش می‌کند.

3.  **محاسبه گرادیان (Gradient Calculation):** در هر مرحله از آموزش، **گرادیان تابع زیان `L` نسبت به هر یک از پارامترهای مدل (وزن `w_j` و بایاس `b`) محاسبه می‌شود**. گرادیان در واقع بردار شیب تابع زیان است و جهت تندترین افزایش تابع را نشان می‌دهد.

4.  **بروزرسانی پارامترها (Parameter Update):** برای کاهش تابع زیان، پارامترها در **جهت مخالف گرادیان** (جهت تندترین کاهش تابع) بروزرسانی می‌شوند. اندازه گامی که در این جهت برداشته می‌شود توسط **نرخ یادگیری (learning rate - `η`)** کنترل می‌شود.
    **قوانین بروزرسانی:**
    `Δw_j = -η * (∂L(w, b) / ∂w_j)`
    `Δb = -η * (∂L(w, b) / ∂b)`
    سپس، پارامترهای جدید به صورت زیر محاسبه می‌شوند:
    `w_j := w_j + Δw_j`
    `b := b + Δb`
    در این فرمول‌ها، `∂L(w, b) / ∂w_j` و `∂L(w, b) / ∂b` مشتقات جزئی تابع زیان `L` نسبت به وزن `j-ام` و بایاس `b` هستند.

**اهمیت نرخ یادگیری (`η`):**
انتخاب مناسب نرخ یادگیری حیاتی است. اگر `η` خیلی بزرگ باشد، الگوریتم ممکن است از حداقل سراسری "پرش" کند و به همگرایی نرسد یا واگرا شود. اگر `η` خیلی کوچک باشد، همگرایی بسیار کند خواهد بود و آموزش زمان زیادی می‌برد.

**انواع کاهنده گرادیان:**

*   **گرادیان دیسنت دسته‌ای کامل (Full Batch Gradient Descent):** گرادیان بر اساس کل مجموعه داده آموزشی محاسبه می‌شود. این روش دقیق است اما برای مجموعه داده‌های بزرگ از نظر محاسباتی پرهزینه است.
*   **گرادیان دیسنت تصادفی (Stochastic Gradient Descent - SGD):** گرادیان تنها بر اساس یک نمونه آموزشی واحد در هر مرحله محاسبه می‌شود. این روش سریع‌تر است اما دارای نویز بیشتری در بروزرسانی‌ها است.
*   **گرادیان دیسنت مینی‌بچ (Mini-batch Gradient Descent):** گرادیان بر اساس یک زیرمجموعه کوچک (mini-batch) از داده‌های آموزشی محاسبه می‌شود. این روش تعادلی بین سرعت SGD و دقت Batch Gradient Descent برقرار می‌کند و معمولاً در عمل ترجیح داده می‌شود. این نویز اضافی در SGD و mini-batch می‌تواند به فرار از حداقل‌های محلی در توابع زیان غیرمحدب نیز کمک کند.

با دنبال کردن این مراحل، می‌توان پارامترهای بهینه برای مدل رگرسیون لجستیک را پیدا کرد و آن را برای طبقه‌بندی داده‌های ارائه شده در جدول آموزش داد.
