این متن به صورت Markdown برای GitHub آماده شده است.

---

# رگرسیون لجستیک و کاهنده گرادیان: حل گام به گام

در پاسخ به پرسش شما در مورد **سوال ۱ منبع exam1** و با تشریح کامل مراحل حل ریاضی به همراه محاسبات گام به گام و ارائه جواب نهایی، از داده‌های ارائه شده در **منبع [۲]** استفاده می‌کنیم:

| A ($x_1$) | B ($x_2$) | Label (y) |
| :-------: | :-------: | :-------: |
| 1 | 0 | 0 |
| 0 | 1 | 0 |
| -1 | 0 | 1 |
| 0 | -1 | 1 |

هدف این است که با استفاده از **طبقه‌بندی رگرسیون لجستیک (Logistic Regression)**، ضرایب ($w$ و $b$) را از طریق **کاهنده گرادیان (Gradient Descent)** محاسبه کنیم [۱].

---

## الف) طبقه‌بندی رگرسیون لجستیک

رگرسیون لجستیک احتمال تعلق یک نمونه به کلاس مثبت (۱) را تخمین می‌زند [۹۰, ۸۴]. مراحل آن:

1.  **ورودی خالص (Net Input) - z:**
    $z = w_1 * x_1 + w_2 * x_2 + b$ [۴۱]
2.  **تابع سیگموئید (Sigmoid Function) - $\sigma(z)$:**
    $\sigma(z) = \frac{1}{(1 + e^{-z})}$ [۸۸]

    این تابع خروجی $z$ را به مقادیر احتمالی بین ۰ و ۱ تبدیل می‌کند [۸۸]. $\sigma(z)$ به عنوان $p(y=1|x)$ (احتمال تعلق به کلاس ۱) تفسیر می‌شود [۹۰].
3.  **تصمیم‌گیری طبقه‌بندی:**
    اگر $\sigma(z) \geq 0.5$ باشد، مدل پیش‌بینی می‌کند که نمونه متعلق به کلاس ۱ است [۹۱]. در غیر این صورت، مدل پیش‌بینی می‌کند که نمونه متعلق به کلاس ۰ است [۹۱]. این معادل است با: اگر $z \geq 0$، $y_{pred} = 1$ و در غیر این صورت $y_{pred} = 0$ [۹۱].
4.  **تابع زیان (Loss Function) - کراس‌انتروپی لجستیک:**
    برای آموزش مدل، تابع زیان کراس‌انتروپی را به حداقل می‌رسانیم [۹۲, ۹۴]:
    $L(w, b) = \Sigma[−y^{(i)} \log(\sigma(z^{(i)})) − (1 − y^{(i)}) \log(1 − \sigma(z^{(i)}))]$ [۹۴]
    این تابع پیش‌بینی‌های نادرست را با زیان بیشتری جریمه می‌کند [۹۹].

---

## ب) منظم‌سازی (Regularization)

**منظم‌سازی** روشی برای مقابله با **بیش‌برازش (overfitting)** است که مدل را قادر می‌سازد تا به داده‌های جدید نیز خوب تعمیم یابد [۷۴, ۱۲۲]. این کار با افزودن یک جمله جریمه (penalty term) به تابع زیان اصلی انجام می‌شود [۷۴].

**انواع متداول منظم‌سازی:**
* **منظم‌سازی L1 (LASSO):** جمله جریمه متناسب با مجموع قدر مطلق وزن‌ها است ($\lambda * \Sigma|w_j|$). این روش می‌تواند برخی از وزن‌ها را به صفر برساند و به انتخاب ویژگی کمک می‌کند [۱۳۶].
* **منظم‌سازی L2 (Ridge Regression):** جمله جریمه متناسب با مجموع مربع وزن‌ها است ($\lambda * \Sigma(w_j)^2$). این روش وزن‌ها را به سمت صفر کوچک می‌کند اما معمولاً آن‌ها را کاملاً صفر نمی‌کند [۱۳۵, ۱۳۶].

**پارامتر منظم‌سازی ($\lambda$ یا C):**
قدرت منظم‌سازی توسط یک **هایپرپارامتر** کنترل می‌شود [۷۴, ۱۰۵]. افزایش قدرت منظم‌سازی (افزایش $\lambda$ یا کاهش C) **بایاس** مدل را افزایش و **واریانس (بیش‌برازش)** را کاهش می‌دهد [۱۱۰].

---

## ج) محاسبه ضرایب با استفاده از کاهنده گرادیان (Gradient Descent)

کاهنده گرادیان یک **الگوریتم بهینه‌سازی تکراری** است که برای یافتن پارامترهای مدل (وزن‌ها $w$ و بایاس $b$) که تابع زیان را به حداقل می‌رسانند، استفاده می‌شود [۶۳, ۳۰۴].

**مراحل کلی الگوریتم:**
1.  **مقداردهی اولیه پارامترها:** وزن‌ها و بایاس با مقادیر اولیه (معمولاً صفر یا اعداد تصادفی کوچک) مقداردهی می‌شوند [۴۳, ۵۲].
2.  **تکرار (Epochs):** الگوریتم برای تعداد مشخصی از تکرارها اجرا می‌شود [۴۳].
3.  **بروزرسانی پارامترها:** پارامترها در جهت مخالف گرادیان تابع زیان (جهت تندترین کاهش تابع) بروزرسانی می‌شوند [۶۵, ۳۰۴]. **نرخ یادگیری ($\eta$)** اندازه این گام را کنترل می‌کند [۴۴].

**فرمول‌های بروزرسانی (بدون منظم‌سازی برای سادگی محاسبات دستی) [۲, ۱۰۳]:**
$\frac{\partial L}{\partial w_j} = \Sigma[(\sigma(z^{(i)}) - y^{(i)}) * x_j^{(i)}]$
$\frac{\partial L}{\partial b} = \Sigma[(\sigma(z^{(i)}) - y^{(i)})]$

$w_j := w_j - \eta * \frac{\partial L}{\partial w_j}$
$b := b - \eta * \frac{\partial L}{\partial b}$

---

## محاسبات گام به گام و جواب نهایی

برای انجام محاسبات، فرضیات زیر را در نظر می‌گیریم:
* **مقادیر اولیه پارامترها:** $w_1 = 0$، $w_2 = 0$، $b = 0$ [۴۳, ۵۲]
* **نرخ یادگیری (Learning Rate):** $\eta = 0.1$ (یک مقدار رایج در مثال‌ها [۴۹, ۶۸, ۷۵])
* **منظم‌سازی:** در این محاسبات گام به گام، برای سادگی و وضوح، جمله جریمه منظم‌سازی را در نظر نمی‌گیریم. (اگر منظم‌سازی L2 اعمال می‌شد، جملات $2 * \lambda * w_j$ به مشتقات $w_j$ اضافه می‌شدند [۱۰۵].)

**داده‌ها:**
| Sample (i) | $x_1$ | $x_2$ | y |
| :---------: | :-: | :-: | :-: |
| 1 | 1 | 0 | 0 |
| 2 | 0 | 1 | 0 |
| 3 | -1 | 0 | 1 |
| 4 | 0 | -1 | 1 |

### **Epoch 1:**

**گام ۱: محاسبه $z^{(i)}$ و $\sigma(z^{(i)})$ و $(\sigma(z^{(i)}) - y^{(i)})$ برای هر نمونه**
* **نمونه ۱ ($x_1 = 1, x_2 = 0, y = 0$):**
    $z^{(1)} = (0 * 1) + (0 * 0) + 0 = 0$
    $\sigma(z^{(1)}) = \frac{1}{(1 + e^0)} = \frac{1}{(1 + 1)} = 0.5$
    $Error\_term^{(1)} = \sigma(z^{(1)}) - y^{(1)} = 0.5 - 0 = 0.5$
* **نمونه ۲ ($x_1 = 0, x_2 = 1, y = 0$):**
    $z^{(2)} = (0 * 0) + (0 * 1) + 0 = 0$
    $\sigma(z^{(2)}) = \frac{1}{(1 + e^0)} = 0.5$
    $Error\_term^{(2)} = \sigma(z^{(2)}) - y^{(2)} = 0.5 - 0 = 0.5$
* **نمونه ۳ ($x_1 = -1, x_2 = 0, y = 1$):**
    $z^{(3)} = (0 * -1) + (0 * 0) + 0 = 0$
    $\sigma(z^{(3)}) = \frac{1}{(1 + e^0)} = 0.5$
    $Error\_term^{(3)} = \sigma(z^{(3)}) - y^{(3)} = 0.5 - 1 = -0.5$
* **نمونه ۴ ($x_1 = 0, x_2 = -1, y = 1$):**
    $z^{(4)} = (0 * 0) + (0 * -1) + 0 = 0$
    $\sigma(z^{(4)}) = \frac{1}{(1 + e^0)} = 0.5$
    $Error\_term^{(4)} = \sigma(z^{(4)}) - y^{(4)} = 0.5 - 1 = -0.5$

**گام ۲: محاسبه گرادیان‌ها**
$\frac{\partial L}{\partial w_1} = (0.5 * 1) + (0.5 * 0) + (-0.5 * -1) + (-0.5 * 0) = 0.5 + 0 + 0.5 + 0 = 1.0$
$\frac{\partial L}{\partial w_2} = (0.5 * 0) + (0.5 * 1) + (-0.5 * 0) + (-0.5 * -1) = 0 + 0.5 + 0 + 0.5 = 1.0$
$\frac{\partial L}{\partial b} = 0.5 + 0.5 + (-0.5) + (-0.5) = 0$

**گام ۳: بروزرسانی پارامترها**
$w_1 := w_1 - \eta * \frac{\partial L}{\partial w_1} = 0 - (0.1 * 1.0) = -0.1$
$w_2 := w_2 - \eta * \frac{\partial L}{\partial w_2} = 0 - (0.1 * 1.0) = -0.1$
$b := b - \eta * \frac{\partial L}{\partial b} = 0 - (0.1 * 0) = 0$

**پارامترهای پس از Epoch 1:**
* $w_1 = -0.1$
* $w_2 = -0.1$
* $b = 0$

### **Epoch 2:**

**گام ۱: محاسبه $z^{(i)}$ و $\sigma(z^{(i)})$ و $(\sigma(z^{(i)}) - y^{(i)})$ برای هر نمونه (با استفاده از $w_1 = -0.1, w_2 = -0.1, b = 0$)**
* **نمونه ۱ ($x_1 = 1, x_2 = 0, y = 0$):**
    $z^{(1)} = (-0.1 * 1) + (-0.1 * 0) + 0 = -0.1$
    $\sigma(z^{(1)}) = \frac{1}{(1 + e^{-(-0.1)})} = \frac{1}{(1 + e^{0.1})} \approx \frac{1}{(1 + 1.105)} \approx 0.475$
    $Error\_term^{(1)} = 0.475 - 0 = 0.475$
* **نمونه ۲ ($x_1 = 0, x_2 = 1, y = 0$):**
    $z^{(2)} = (-0.1 * 0) + (-0.1 * 1) + 0 = -0.1$
    $\sigma(z^{(2)}) \approx 0.475$
    $Error\_term^{(2)} = 0.475 - 0 = 0.475$
* **نمونه ۳ ($x_1 = -1, x_2 = 0, y = 1$):**
    $z^{(3)} = (-0.1 * -1) + (-0.1 * 0) + 0 = 0.1$
    $\sigma(z^{(3)}) = \frac{1}{(1 + e^{-0.1})} \approx \frac{1}{(1 + 0.905)} \approx 0.525$
    $Error\_term^{(3)} = 0.525 - 1 = -0.475$
* **نمونه ۴ ($x_1 = 0, x_2 = -1, y = 1$):**
    $z^{(4)} = (-0.1 * 0) + (-0.1 * -1) + 0 = 0.1$
    $\sigma(z^{(4)}) \approx 0.525$
    $Error\_term^{(4)} = 0.525 - 1 = -0.475$

**گام ۲: محاسبه گرادیان‌ها**
$\frac{\partial L}{\partial w_1} = (0.475 * 1) + (0.475 * 0) + (-0.475 * -1) + (-0.475 * 0) = 0.475 + 0 + 0.475 + 0 = 0.95$
$\frac{\partial L}{\partial w_2} = (0.475 * 0) + (0.475 * 1) + (-0.475 * 0) + (-0.475 * -1) = 0 + 0.475 + 0 + 0.475 = 0.95$
$\frac{\partial L}{\partial b} = 0.475 + 0.475 + (-0.475) + (-0.475) = 0$

**گام ۳: بروزرسانی پارامترها**
$w_1 := w_1 - \eta * \frac{\partial L}{\partial w_1} = -0.1 - (0.1 * 0.95) = -0.1 - 0.095 = -0.195$
$w_2 := w_2 - \eta * \frac{\partial L}{\partial w_2} = -0.1 - (0.1 * 0.95) = -0.1 - 0.095 = -0.195$
$b := b - \eta * \frac{\partial L}{\partial b} = 0 - (0.1 * 0) = 0$

**پارامترهای پس از Epoch 2:**
* $w_1 = -0.195$
* $w_2 = -0.195$
* $b = 0$

---

## جواب نهایی و تحلیل

با توجه به ماهیت تکراری الگوریتم کاهنده گرادیان، "جواب نهایی" به معنای مقادیری از پارامترها است که پس از تعداد کافی تکرار (epochs) به یک مقدار حداقل همگرا شده‌اند [۴۹, ۶۳]. در اینجا، پس از دو دور تکرار، مقادیر ضرایب به شرح زیر است:

* **ضریب $w_1$ (برای ویژگی A) = -0.195**
* **ضریب $w_2$ (برای ویژگی B) = -0.195**
* **بایاس $b$ = 0**

همانطور که مشاهده می‌شود، وزن‌ها $w_1$ و $w_2$ در حال کاهش (منفی‌تر شدن) هستند، در حالی که بایاس $b$ همچنان صفر باقی مانده است. این نشان‌دهنده همگرایی تدریجی پارامترها در جهت کاهش تابع زیان است [۶۹]. صفر ماندن بایاس در این مثال خاص به دلیل تقارن داده‌ها حول محور y=0 و مقادیر اولیه صفر برای وزن‌ها و بایاس است، که منجر به مشتق صفر بایاس در هر گام می‌شود. در یک اجرای کامل، این فرآیند برای تعداد مشخصی از epochs (مثلاً ۵۰ یا ۱۰۰۰ دور) ادامه می‌یابد تا پارامترها به مقادیر بهینه همگرا شوند [۴۹, ۱۰۱].

برای افزایش دقت و جلوگیری از بیش‌برازش، می‌توان از تکنیک‌های منظم‌سازی (مانند L1 یا L2) استفاده کرد. در صورت استفاده از منظم‌سازی L2، قوانین بروزرسانی وزن‌ها ($w_j$) شامل یک جمله اضافی $2 * \lambda * w_j$ در گرادیان خواهد بود، که وزن‌ها را به سمت صفر کوچک می‌کند [۱۰۵, ۱۳۶]. انتخاب مناسب نرخ یادگیری نیز برای همگرایی بهینه بسیار مهم است؛ نرخ یادگیری بیش از حد بزرگ می‌تواند باعث پرش از حداقل شود، در حالی که نرخ یادگیری بسیار کوچک باعث کندی همگرایی می‌شود [۶۹, ۷۰].





---

**سوال ۲ –**  
داده‌های جدول زیر را در نظر بگیرید. با استفاده از روش **ماشین بردار پشتیبان (SVM)** برای طبقه‌بندی داده‌ها، به سؤالات زیر پاسخ دهید.

### جدول داده‌ها:

| \( i \)       | \( X_1 \) | \( X_2 \) | \( Y \) | \( \alpha_i \) |
|---------------|----------|----------|--------|----------------|
| \( x_1 \)     | 8.0      | 5.8      | 1      | 0.414          |
| \( x_2 \)     | 8.0      | 8.0      | 1      | 0.000          |
| \( x_3 \)     | 2.0      | 5.0      | -1     | 0.000          |
| \( x_4 \)     | 5.0      | 2.0      | -1     | 0.018          |
| \( x_5 \)     | 9.8      | 9.0      | 1      | 0.000          |
| \( x_6 \)     | 3.8      | 3.8      | -1     | 0.000          |
| \( x_7 \)     | 7.0      | 8.0      | 1      | 0.018          |
| \( x_8 \)     | 1.0      | 3.0      | -1     | 0.000          |
| \( x_9 \)     | 4.0      | 4.2      | -1     | 0.414          |
| \( x_{10} \)  | 9.0      | 5.0      | 1      | 0.000          |

---

### فرضیات:
- مدل SVM با **تابع تصمیم خطی** \( h(\mathbf{x}) \) آموزش داده شده است.
- هدف، محاسبه **مارژین (Margin)** و بررسی وضعیت نقاط نسبت به مرز تصمیم است.
- همچنین، مقدار \( h(\mathbf{x}) \) برای نقطهٔ تست \( \mathbf{x} = (3, 3)^T \) باید محاسبه شود.

---

### سؤالات:

**الف)** تابع تصمیم \( h(\mathbf{x}) \) را برای این مدل SVM به دست آورید.  
نحوه محاسبه آن را با استفاده از فرمول زیر و داده‌های جدول (به‌ویژه نقاط با \( \alpha_i > 0 \)) توضیح دهید:

\[
h(\mathbf{x}) = \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i^T \mathbf{x} + b
\]

در اینجا، فقط نقاطی که \( \alpha_i > 0 \) هستند (یعنی **بردارهای پشتیبان**) در محاسبه تأثیر دارند.

---

**ب)** فاصله نقطه \( x_6 = (3.8, 3.8)^T \) نسبت به مرز تصمیم (یعنی مقدار **مارژین نسبی** \( \delta \)) را محاسبه کنید.  
آیا این نقطه یک **بردار پشتیبان (Support Vector)** است؟ چرا؟

راهنمایی: از فرمول زیر استفاده کنید:

\[
\delta = \frac{y_i \cdot h(\mathbf{x}_i)}{\|\mathbf{w}\|}
\]

---

**پ)** مقدار تابع تصمیم \( h(\mathbf{x}) \) را برای نقطهٔ تست \( \mathbf{x} = (3, 3)^T \) محاسبه کنید.  
سپس، برچسب پیش‌بینی‌شدهٔ آن (مثبت یا منفی) را مشخص کنید.

---

✅ **نکته:**  
- بردار وزن \( \mathbf{w} \) از رابطه \( \mathbf{w} = \sum \alpha_i y_i \mathbf{x}_i \) به دست می‌آید.  
- مقدار \( b \) (بایاس) را می‌توان با استفاده از یکی از بردارهای پشتیبان (مثلاً \( x_1 \) یا \( x_9 \)) و رابطه \( h(\mathbf{x}_i) = y_i \) محاسبه کرد.

---
```markdown
### الف) تابع تصمیم \( h(\mathbf{x}) \)

تابع تصمیم برای یک SVM خطی به شکل  
\[
h(\mathbf{x}) = \mathbf{w}^\mathsf{T}\mathbf{x} + b
\]  
تعریف می‌شود. بردار وزن \( \mathbf{w} \) و بایاس \( b \) با استفاده از بردارهای پشتیبان و ضرایب \( \alpha_i \) محاسبه می‌شوند.

#### شناسایی بردارهای پشتیبان
نمونه‌هایی که \( \alpha_i > 0 \) هستند:

| نقطه | مختصات | برچسب \( y_i \) | \( \alpha_i \) |
|------|---------|-----------------|----------------|
| \( \mathbf{x}_1 \) | \( (8, 5.8) \) | \( +1 \) | 0.414 |
| \( \mathbf{x}_4 \) | \( (5, 2) \)   | \( -1 \) | 0.018 |
| \( \mathbf{x}_7 \) | \( (7, 8) \)   | \( +1 \) | 0.018 |
| \( \mathbf{x}_9 \) | \( (4, 4.2) \) | \( -1 \) | 0.414 |

#### محاسبه بردار وزن \( \mathbf{w} \)
\[
\mathbf{w} = \sum_{i \in \text{SV}} \alpha_i y_i \mathbf{x}_i
\]

\[
\begin{aligned}
\mathbf{w} &= 0.414(+1)\begin{pmatrix}8 \\ 5.8\end{pmatrix}
            + 0.018(-1)\begin{pmatrix}5 \\ 2\end{pmatrix}
            + 0.018(+1)\begin{pmatrix}7 \\ 8\end{pmatrix}
            + 0.414(-1)\begin{pmatrix}4 \\ 4.2\end{pmatrix} \\[4pt]
&= \begin{pmatrix}3.312 \\ 2.3952\end{pmatrix}
 + \begin{pmatrix}-0.090 \\ -0.036\end{pmatrix}
 + \begin{pmatrix}0.126 \\ 0.144\end{pmatrix}
 + \begin{pmatrix}-1.656 \\ -1.7388\end{pmatrix} \\[4pt]
&= \begin{pmatrix}1.692 \\ 0.7644\end{pmatrix}
\end{aligned}
\]

#### محاسبه بایاس \( b \)
با استفاده از \( \mathbf{x}_4 \) (که \( \alpha \) کوچک‌تری دارد):

\[
-1 \cdot \bigl(1.692 \cdot 5 + 0.7644 \cdot 2 + b\bigr) = 1
\Longrightarrow b = -10.9888
\]

بنابراین تابع تصمیم:

\[
\boxed{h(\mathbf{x}) = 1.692\,x_1 + 0.7644\,x_2 - 10.9888}
\]

---

### ب) فاصله (مارژین) نقطه \( \mathbf{x}_6 \)

- **وضعیت \( \mathbf{x}_6 \):**  
  \( \alpha_6 = 0 \Rightarrow \) **بردار پشتیبان نیست**.

- **محاسبه \( h(\mathbf{x}_6) \):**  
  \[
  h\!\bigl((3.8, 3.8)\bigr) = 1.692 \cdot 3.8 + 0.7644 \cdot 3.8 - 10.9888 \approx -1.6545
  \]

- **نرم بردار وزن:**  
  \[
  \|\mathbf{w}\| = \sqrt{1.692^2 + 0.7644^2} \approx 1.8566
  \]

- **مارژین:**  
  \[
  \delta_6 = \frac{(-1)\cdot(-1.6545)}{1.8566} \approx 0.8911
  \]

\[
\boxed{\delta_6 \approx 0.891 \quad (\text{SV نیست})}
\]

---

### پ) طبقه‌بندی نقطه \( (3,3)^\mathsf{T} \)

\[
h\!\bigl((3,3)\bigr) = 1.692 \cdot 3 + 0.7644 \cdot 3 - 10.9888 \approx -3.6196 < 0
\]

\[
\boxed{(3,3) \text{ در کلاس } -1 \text{ قرار می‌گیرد}}
\]
```
