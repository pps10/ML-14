این متن به صورت Markdown برای GitHub آماده شده است.

---

# رگرسیون لجستیک و کاهنده گرادیان: حل گام به گام

در پاسخ به پرسش شما در مورد **سوال ۱ منبع exam1** و با تشریح کامل مراحل حل ریاضی به همراه محاسبات گام به گام و ارائه جواب نهایی، از داده‌های ارائه شده در **منبع [۲]** استفاده می‌کنیم:

| A ($x_1$) | B ($x_2$) | Label (y) |
| :-------: | :-------: | :-------: |
| 1 | 0 | 0 |
| 0 | 1 | 0 |
| -1 | 0 | 1 |
| 0 | -1 | 1 |

هدف این است که با استفاده از **طبقه‌بندی رگرسیون لجستیک (Logistic Regression)**، ضرایب ($w$ و $b$) را از طریق **کاهنده گرادیان (Gradient Descent)** محاسبه کنیم [۱].

---

## الف) طبقه‌بندی رگرسیون لجستیک

رگرسیون لجستیک احتمال تعلق یک نمونه به کلاس مثبت (۱) را تخمین می‌زند [۹۰, ۸۴]. مراحل آن:

1.  **ورودی خالص (Net Input) - z:**
    $z = w_1 * x_1 + w_2 * x_2 + b$ [۴۱]
2.  **تابع سیگموئید (Sigmoid Function) - $\sigma(z)$:**
    $\sigma(z) = \frac{1}{(1 + e^{-z})}$ [۸۸]

    این تابع خروجی $z$ را به مقادیر احتمالی بین ۰ و ۱ تبدیل می‌کند [۸۸]. $\sigma(z)$ به عنوان $p(y=1|x)$ (احتمال تعلق به کلاس ۱) تفسیر می‌شود [۹۰].
3.  **تصمیم‌گیری طبقه‌بندی:**
    اگر $\sigma(z) \geq 0.5$ باشد، مدل پیش‌بینی می‌کند که نمونه متعلق به کلاس ۱ است [۹۱]. در غیر این صورت، مدل پیش‌بینی می‌کند که نمونه متعلق به کلاس ۰ است [۹۱]. این معادل است با: اگر $z \geq 0$، $y_{pred} = 1$ و در غیر این صورت $y_{pred} = 0$ [۹۱].
4.  **تابع زیان (Loss Function) - کراس‌انتروپی لجستیک:**
    برای آموزش مدل، تابع زیان کراس‌انتروپی را به حداقل می‌رسانیم [۹۲, ۹۴]:
    $L(w, b) = \Sigma[−y^{(i)} \log(\sigma(z^{(i)})) − (1 − y^{(i)}) \log(1 − \sigma(z^{(i)}))]$ [۹۴]
    این تابع پیش‌بینی‌های نادرست را با زیان بیشتری جریمه می‌کند [۹۹].

---

## ب) منظم‌سازی (Regularization)

**منظم‌سازی** روشی برای مقابله با **بیش‌برازش (overfitting)** است که مدل را قادر می‌سازد تا به داده‌های جدید نیز خوب تعمیم یابد [۷۴, ۱۲۲]. این کار با افزودن یک جمله جریمه (penalty term) به تابع زیان اصلی انجام می‌شود [۷۴].

**انواع متداول منظم‌سازی:**
* **منظم‌سازی L1 (LASSO):** جمله جریمه متناسب با مجموع قدر مطلق وزن‌ها است ($\lambda * \Sigma|w_j|$). این روش می‌تواند برخی از وزن‌ها را به صفر برساند و به انتخاب ویژگی کمک می‌کند [۱۳۶].
* **منظم‌سازی L2 (Ridge Regression):** جمله جریمه متناسب با مجموع مربع وزن‌ها است ($\lambda * \Sigma(w_j)^2$). این روش وزن‌ها را به سمت صفر کوچک می‌کند اما معمولاً آن‌ها را کاملاً صفر نمی‌کند [۱۳۵, ۱۳۶].

**پارامتر منظم‌سازی ($\lambda$ یا C):**
قدرت منظم‌سازی توسط یک **هایپرپارامتر** کنترل می‌شود [۷۴, ۱۰۵]. افزایش قدرت منظم‌سازی (افزایش $\lambda$ یا کاهش C) **بایاس** مدل را افزایش و **واریانس (بیش‌برازش)** را کاهش می‌دهد [۱۱۰].

---

## ج) محاسبه ضرایب با استفاده از کاهنده گرادیان (Gradient Descent)

کاهنده گرادیان یک **الگوریتم بهینه‌سازی تکراری** است که برای یافتن پارامترهای مدل (وزن‌ها $w$ و بایاس $b$) که تابع زیان را به حداقل می‌رسانند، استفاده می‌شود [۶۳, ۳۰۴].

**مراحل کلی الگوریتم:**
1.  **مقداردهی اولیه پارامترها:** وزن‌ها و بایاس با مقادیر اولیه (معمولاً صفر یا اعداد تصادفی کوچک) مقداردهی می‌شوند [۴۳, ۵۲].
2.  **تکرار (Epochs):** الگوریتم برای تعداد مشخصی از تکرارها اجرا می‌شود [۴۳].
3.  **بروزرسانی پارامترها:** پارامترها در جهت مخالف گرادیان تابع زیان (جهت تندترین کاهش تابع) بروزرسانی می‌شوند [۶۵, ۳۰۴]. **نرخ یادگیری ($\eta$)** اندازه این گام را کنترل می‌کند [۴۴].

**فرمول‌های بروزرسانی (بدون منظم‌سازی برای سادگی محاسبات دستی) [۲, ۱۰۳]:**
$\frac{\partial L}{\partial w_j} = \Sigma[(\sigma(z^{(i)}) - y^{(i)}) * x_j^{(i)}]$
$\frac{\partial L}{\partial b} = \Sigma[(\sigma(z^{(i)}) - y^{(i)})]$

$w_j := w_j - \eta * \frac{\partial L}{\partial w_j}$
$b := b - \eta * \frac{\partial L}{\partial b}$

---

## محاسبات گام به گام و جواب نهایی

برای انجام محاسبات، فرضیات زیر را در نظر می‌گیریم:
* **مقادیر اولیه پارامترها:** $w_1 = 0$، $w_2 = 0$، $b = 0$ [۴۳, ۵۲]
* **نرخ یادگیری (Learning Rate):** $\eta = 0.1$ (یک مقدار رایج در مثال‌ها [۴۹, ۶۸, ۷۵])
* **منظم‌سازی:** در این محاسبات گام به گام، برای سادگی و وضوح، جمله جریمه منظم‌سازی را در نظر نمی‌گیریم. (اگر منظم‌سازی L2 اعمال می‌شد، جملات $2 * \lambda * w_j$ به مشتقات $w_j$ اضافه می‌شدند [۱۰۵].)

**داده‌ها:**
| Sample (i) | $x_1$ | $x_2$ | y |
| :---------: | :-: | :-: | :-: |
| 1 | 1 | 0 | 0 |
| 2 | 0 | 1 | 0 |
| 3 | -1 | 0 | 1 |
| 4 | 0 | -1 | 1 |

### **Epoch 1:**

**گام ۱: محاسبه $z^{(i)}$ و $\sigma(z^{(i)})$ و $(\sigma(z^{(i)}) - y^{(i)})$ برای هر نمونه**
* **نمونه ۱ ($x_1 = 1, x_2 = 0, y = 0$):**
    $z^{(1)} = (0 * 1) + (0 * 0) + 0 = 0$
    $\sigma(z^{(1)}) = \frac{1}{(1 + e^0)} = \frac{1}{(1 + 1)} = 0.5$
    $Error\_term^{(1)} = \sigma(z^{(1)}) - y^{(1)} = 0.5 - 0 = 0.5$
* **نمونه ۲ ($x_1 = 0, x_2 = 1, y = 0$):**
    $z^{(2)} = (0 * 0) + (0 * 1) + 0 = 0$
    $\sigma(z^{(2)}) = \frac{1}{(1 + e^0)} = 0.5$
    $Error\_term^{(2)} = \sigma(z^{(2)}) - y^{(2)} = 0.5 - 0 = 0.5$
* **نمونه ۳ ($x_1 = -1, x_2 = 0, y = 1$):**
    $z^{(3)} = (0 * -1) + (0 * 0) + 0 = 0$
    $\sigma(z^{(3)}) = \frac{1}{(1 + e^0)} = 0.5$
    $Error\_term^{(3)} = \sigma(z^{(3)}) - y^{(3)} = 0.5 - 1 = -0.5$
* **نمونه ۴ ($x_1 = 0, x_2 = -1, y = 1$):**
    $z^{(4)} = (0 * 0) + (0 * -1) + 0 = 0$
    $\sigma(z^{(4)}) = \frac{1}{(1 + e^0)} = 0.5$
    $Error\_term^{(4)} = \sigma(z^{(4)}) - y^{(4)} = 0.5 - 1 = -0.5$

**گام ۲: محاسبه گرادیان‌ها**
$\frac{\partial L}{\partial w_1} = (0.5 * 1) + (0.5 * 0) + (-0.5 * -1) + (-0.5 * 0) = 0.5 + 0 + 0.5 + 0 = 1.0$
$\frac{\partial L}{\partial w_2} = (0.5 * 0) + (0.5 * 1) + (-0.5 * 0) + (-0.5 * -1) = 0 + 0.5 + 0 + 0.5 = 1.0$
$\frac{\partial L}{\partial b} = 0.5 + 0.5 + (-0.5) + (-0.5) = 0$

**گام ۳: بروزرسانی پارامترها**
$w_1 := w_1 - \eta * \frac{\partial L}{\partial w_1} = 0 - (0.1 * 1.0) = -0.1$
$w_2 := w_2 - \eta * \frac{\partial L}{\partial w_2} = 0 - (0.1 * 1.0) = -0.1$
$b := b - \eta * \frac{\partial L}{\partial b} = 0 - (0.1 * 0) = 0$

**پارامترهای پس از Epoch 1:**
* $w_1 = -0.1$
* $w_2 = -0.1$
* $b = 0$

### **Epoch 2:**

**گام ۱: محاسبه $z^{(i)}$ و $\sigma(z^{(i)})$ و $(\sigma(z^{(i)}) - y^{(i)})$ برای هر نمونه (با استفاده از $w_1 = -0.1, w_2 = -0.1, b = 0$)**
* **نمونه ۱ ($x_1 = 1, x_2 = 0, y = 0$):**
    $z^{(1)} = (-0.1 * 1) + (-0.1 * 0) + 0 = -0.1$
    $\sigma(z^{(1)}) = \frac{1}{(1 + e^{-(-0.1)})} = \frac{1}{(1 + e^{0.1})} \approx \frac{1}{(1 + 1.105)} \approx 0.475$
    $Error\_term^{(1)} = 0.475 - 0 = 0.475$
* **نمونه ۲ ($x_1 = 0, x_2 = 1, y = 0$):**
    $z^{(2)} = (-0.1 * 0) + (-0.1 * 1) + 0 = -0.1$
    $\sigma(z^{(2)}) \approx 0.475$
    $Error\_term^{(2)} = 0.475 - 0 = 0.475$
* **نمونه ۳ ($x_1 = -1, x_2 = 0, y = 1$):**
    $z^{(3)} = (-0.1 * -1) + (-0.1 * 0) + 0 = 0.1$
    $\sigma(z^{(3)}) = \frac{1}{(1 + e^{-0.1})} \approx \frac{1}{(1 + 0.905)} \approx 0.525$
    $Error\_term^{(3)} = 0.525 - 1 = -0.475$
* **نمونه ۴ ($x_1 = 0, x_2 = -1, y = 1$):**
    $z^{(4)} = (-0.1 * 0) + (-0.1 * -1) + 0 = 0.1$
    $\sigma(z^{(4)}) \approx 0.525$
    $Error\_term^{(4)} = 0.525 - 1 = -0.475$

**گام ۲: محاسبه گرادیان‌ها**
$\frac{\partial L}{\partial w_1} = (0.475 * 1) + (0.475 * 0) + (-0.475 * -1) + (-0.475 * 0) = 0.475 + 0 + 0.475 + 0 = 0.95$
$\frac{\partial L}{\partial w_2} = (0.475 * 0) + (0.475 * 1) + (-0.475 * 0) + (-0.475 * -1) = 0 + 0.475 + 0 + 0.475 = 0.95$
$\frac{\partial L}{\partial b} = 0.475 + 0.475 + (-0.475) + (-0.475) = 0$

**گام ۳: بروزرسانی پارامترها**
$w_1 := w_1 - \eta * \frac{\partial L}{\partial w_1} = -0.1 - (0.1 * 0.95) = -0.1 - 0.095 = -0.195$
$w_2 := w_2 - \eta * \frac{\partial L}{\partial w_2} = -0.1 - (0.1 * 0.95) = -0.1 - 0.095 = -0.195$
$b := b - \eta * \frac{\partial L}{\partial b} = 0 - (0.1 * 0) = 0$

**پارامترهای پس از Epoch 2:**
* $w_1 = -0.195$
* $w_2 = -0.195$
* $b = 0$

---

## جواب نهایی و تحلیل

با توجه به ماهیت تکراری الگوریتم کاهنده گرادیان، "جواب نهایی" به معنای مقادیری از پارامترها است که پس از تعداد کافی تکرار (epochs) به یک مقدار حداقل همگرا شده‌اند [۴۹, ۶۳]. در اینجا، پس از دو دور تکرار، مقادیر ضرایب به شرح زیر است:

* **ضریب $w_1$ (برای ویژگی A) = -0.195**
* **ضریب $w_2$ (برای ویژگی B) = -0.195**
* **بایاس $b$ = 0**

همانطور که مشاهده می‌شود، وزن‌ها $w_1$ و $w_2$ در حال کاهش (منفی‌تر شدن) هستند، در حالی که بایاس $b$ همچنان صفر باقی مانده است. این نشان‌دهنده همگرایی تدریجی پارامترها در جهت کاهش تابع زیان است [۶۹]. صفر ماندن بایاس در این مثال خاص به دلیل تقارن داده‌ها حول محور y=0 و مقادیر اولیه صفر برای وزن‌ها و بایاس است، که منجر به مشتق صفر بایاس در هر گام می‌شود. در یک اجرای کامل، این فرآیند برای تعداد مشخصی از epochs (مثلاً ۵۰ یا ۱۰۰۰ دور) ادامه می‌یابد تا پارامترها به مقادیر بهینه همگرا شوند [۴۹, ۱۰۱].

برای افزایش دقت و جلوگیری از بیش‌برازش، می‌توان از تکنیک‌های منظم‌سازی (مانند L1 یا L2) استفاده کرد. در صورت استفاده از منظم‌سازی L2، قوانین بروزرسانی وزن‌ها ($w_j$) شامل یک جمله اضافی $2 * \lambda * w_j$ در گرادیان خواهد بود، که وزن‌ها را به سمت صفر کوچک می‌کند [۱۰۵, ۱۳۶]. انتخاب مناسب نرخ یادگیری نیز برای همگرایی بهینه بسیار مهم است؛ نرخ یادگیری بیش از حد بزرگ می‌تواند باعث پرش از حداقل شود، در حالی که نرخ یادگیری بسیار کوچک باعث کندی همگرایی می‌شود [۶۹, ۷۰].



سوال ۲ –
داده‌های جدول زیر (جدول داده‌ها) را در نظر بگیرید. با استفاده از روش SVM برای طبقه‌بندی داده‌ها، پاسخ به سؤالات زیر را ارائه دهید.


جدول داده‌ها:

i	X1	X2	Y	α
( x_1 )	8	5.8	1	0.414
( x_2 )	8	8	1	0
( x_3 )	2	5	-1	0
( x_4 )	5	2	-1	0.018
( x_5 )	9.8	9	1	0
( x_6 )	3.8	3.8	-1	0
( x_7 )	7	8	1	0.018
( x_8 )	1	3	-1	0
( x_9 )	4	4.2	-1	0.414
( x_{10} )	9	5	1	0

فرض:


مدل SVM با تابع تصمیم ( h(\mathbf{x}) ) است.

هدف، محاسبه مارژین (Margin) و تعیین وضعیت نقاط در مجاورت هипرپلین است.

همچنین، مقدار ( h(x) ) برای نقطه ( x = (3,3)^T ) باید محاسبه شود.


الف) تابع تصمیم ( h(\mathbf{x}) ) را برای SVM بدست آورید و نشان دهید که چطور با استفاده از ضرایب ( \alpha_i ) و داده‌های آموزشی، این تابع محاسبه می‌شود.


ب) فاصله (مقدار مارژین) نقطه ( x_6 ) را محاسبه کنید. آیا این نقطه روی مرز تصمیم (Support Vector) قرار دارد؟


پ) مقدار تابع تصمیم ( h(x) ) را برای نقطه ( x = (3,3)^T ) محاسبه کنید و نوع طبقه‌بندی آن را مشخص نمایید.



فرمول مربوطه:

[
\delta = y \cdot r = \frac{y \cdot h(\mathbf{x})}{|\mathbf{w}|}
]


(که در آن ( \delta ) مقدار مارژین نسبت به نقطه ( x ) است.)

### پاسخ به سؤالات SVM

---

### مقدمه: ماشین بردار پشتیبان (SVM)

ماشین‌های بردار پشتیبان (SVM) از الگوریتم‌های قدرتمند یادگیری برای طبقه‌بندی هستند که هدف اصلی‌شان **حداکثر کردن مارژین** است. مارژین به فاصله میان **ابرصفحه جداکننده** (مرز تصمیم) و نزدیک‌ترین نمونه‌های آموزشی (که به آن‌ها **بردارهای پشتیبان** می‌گویند) اشاره دارد. ضرایب $\alpha_i$ (Lagrange multipliers) که در جدول داده‌ها آمده‌اند، تنها برای بردارهای پشتیبان مقداری غیر صفر دارند.

---

### الف) تابع تصمیم ($h(\mathbf{x})$)

تابع تصمیم $h(\mathbf{x})$ برای یک SVM خطی به شکل $h(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b$ تعریف می‌شود. بردار وزن $\mathbf{w}$ و بایاس $b$ با استفاده از بردارهای پشتیبان و ضرایب $\alpha_i$ محاسبه می‌شوند.

1.  **شناسایی بردارهای پشتیبان**:
    بردارهای پشتیبان نمونه‌هایی هستند که $\alpha_i$ آن‌ها بزرگ‌تر از صفر است. بر اساس داده‌های جدول، این نقاط عبارتند از:
    * $x_1 = (8, 5.8)^T$ با $Y=1$ و $\alpha = 0.414$
    * $x_4 = (5, 2)^T$ با $Y=-1$ و $\alpha = 0.018$
    * $x_7 = (7, 8)^T$ با $Y=1$ و $\alpha = 0.018$
    * $x_9 = (4, 4.2)^T$ با $Y=-1$ و $\alpha = 0.414$

2.  **محاسبه بردار وزن ($\mathbf{w}$)**:
    بردار وزن به صورت $\mathbf{w} = \sum_{i \in SVs} \alpha_i y_i \mathbf{x}_i$ محاسبه می‌شود.
    $\mathbf{w} = (0.414 \times 1 \times \begin{pmatrix} 8 \\ 5.8 \end{pmatrix}) + (0.018 \times -1 \times \begin{pmatrix} 5 \\ 2 \end{pmatrix}) + (0.018 \times 1 \times \begin{pmatrix} 7 \\ 8 \end{pmatrix}) + (0.414 \times -1 \times \begin{pmatrix} 4 \\ 4.2 \end{pmatrix})$
    $\mathbf{w} = \begin{pmatrix} 3.312 \\ 2.3952 \end{pmatrix} + \begin{pmatrix} -0.090 \\ -0.036 \end{pmatrix} + \begin{pmatrix} 0.126 \\ 0.144 \end{pmatrix} + \begin{pmatrix} -1.656 \\ -1.7388 \end{pmatrix} = \begin{pmatrix} 1.692 \\ 0.7644 \end{pmatrix}$

3.  **محاسبه بایاس ($b$)**:
    بایاس $b$ باید به‌گونه‌ای باشد که برای بردارهای پشتیبان، $y_k (\mathbf{w}^T \mathbf{x}_k + b) = 1$ برقرار باشد. به دلیل تفاوت‌های محاسباتی و اینکه داده‌ها ممکن است به صورت خطی کاملاً قابل تفکیک نباشند (soft-margin SVM)، مقادیر $b$ محاسبه شده از هر بردار پشتیبان کمی متفاوت هستند. برای این مسئله، مقدار بایاس را از بردار پشتیبان $x_4$ (که $\alpha$ کوچک‌تری دارد) در نظر می‌گیریم:
    $-1 \times ((1.692 \times 5) + (0.7644 \times 2) + b) = 1$
    $-1 \times (8.460 + 1.5288 + b) = 1$
    $-(9.9888 + b) = 1 \Rightarrow b = -10.9888$
    بنابراین، **تابع تصمیم** به صورت زیر است:
    $h(\mathbf{x}) = 1.692 \times x_1 + 0.7644 \times x_2 - 10.9888$

---

### ب) فاصله (مارژین) نقطه $x_6$

1.  **تعیین وضعیت $x_6$**:
    نقطه $x_6 = (3.8, 3.8)^T$ دارای $\alpha_6 = 0$ است. از آنجایی که $\alpha$ آن صفر است، **این نقطه بردار پشتیبان نیست** و روی مرز تصمیم قرار ندارد.

2.  **محاسبه مقدار $h(x_6)$**:
    $h(x_6) = (1.692 \times 3.8) + (0.7644 \times 3.8) - 10.9888$
    $h(x_6) = 6.4296 + 2.90472 - 10.9888 = -1.65448$

3.  **محاسبه نرم بردار وزن ($|\mathbf{w}|$)**:
    $|\mathbf{w}| = \sqrt{1.692^2 + 0.7644^2} = \sqrt{2.862864 + 0.58408996} = \sqrt{3.44695396} \approx 1.8566$

4.  **محاسبه مارژین ($\delta$)**:
    مارژین با فرمول $\delta = \frac{y \cdot h(\mathbf{x})}{|\mathbf{w}|}$ محاسبه می‌شود. برای $x_6$ که $Y=-1$ است:
    $\delta_6 = \frac{(-1) \times (-1.65448)}{1.8566} = \frac{1.65448}{1.8566} \approx 0.8911$
    از آنجایی که $\delta_6$ بزرگ‌تر از $1/|\mathbf{w}| \approx 0.5386$ است، این نقطه با فاصله خوبی در سمت درست ابرصفحه قرار دارد و **بردار پشتیبان نیست**.

---

### پ) طبقه‌بندی نقطه $x = (3,3)^T$

1.  **محاسبه $h(\mathbf{x})$**:
    برای طبقه‌بندی نقطه $x = (3,3)^T$، مقدار $h(\mathbf{x})$ را محاسبه می‌کنیم:
    $h((3,3)^T) = (1.692 \times 3) + (0.7644 \times 3) - 10.9888$
    $h((3,3)^T) = 5.076 + 2.2932 - 10.9888 = -3.6196$

2.  **تعیین نوع طبقه‌بندی**:
    علامت $h(\mathbf{x})$ نوع طبقه‌بندی را مشخص می‌کند. اگر $h(\mathbf{x}) > 0$ باشد، کلاس $+1$ است و اگر $h(\mathbf{x}) < 0$ باشد، کلاس $-1$ است.
    چون $h((3,3)^T) = -3.6196$ منفی است، این نقطه در **کلاس -1** طبقه‌بندی می‌شود.
